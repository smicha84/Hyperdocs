<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Phase 0 Model Test Results — Plain English</title>
<style>
* { margin:0; padding:0; box-sizing:border-box; }
body { font-family:'Georgia','Times New Roman',serif; background:#0d1117; color:#c9d1d9; padding:40px; line-height:1.9; max-width:900px; margin:0 auto; font-size:16px; }
h1 { color:#58a6ff; font-size:24px; margin-bottom:8px; font-family:'SF Mono',monospace; }
h2 { color:#f0f6fc; font-size:20px; margin:40px 0 16px; padding-bottom:8px; border-bottom:1px solid #30363d; }
h3 { color:#d29922; font-size:17px; margin:28px 0 12px; }
p { margin:14px 0; }
.highlight { color:#f85149; font-weight:600; }
.good { color:#3fb950; font-weight:600; }
.neutral { color:#d29922; }
table { width:100%; border-collapse:collapse; margin:16px 0; font-family:'SF Mono',monospace; font-size:13px; }
th { text-align:left; padding:8px 10px; color:#8b949e; border-bottom:2px solid #30363d; font-size:11px; text-transform:uppercase; }
td { padding:6px 10px; border-bottom:1px solid #21262d; }
tr:hover td { background:#161b22; }
.box { background:#161b22; border:1px solid #30363d; border-radius:8px; padding:20px; margin:20px 0; }
.verdict { background:#0d4429; border:1px solid #2ea043; border-radius:8px; padding:20px; margin:24px 0; }
.verdict h3 { color:#3fb950; margin-top:0; }
.problem { background:#4a1c1c; border:1px solid #f85149; border-radius:8px; padding:20px; margin:24px 0; }
.problem h3 { color:#f85149; margin-top:0; }
</style>
</head>
<body>

<h1>Phase 0 Model Test Results</h1>
<p style="color:#8b949e">Fifty real messages from fifteen different sessions were analyzed by three models — Opus, Sonnet, and Haiku — performing every Phase 0 task. One hundred and fifty API calls total. Here is what happened.</p>

<h2>Did the models actually work?</h2>

<p>All three models returned valid, parseable JSON for every single one of the fifty messages. There were zero errors and zero formatting failures across all one hundred and fifty calls. This means the comprehensive Phase 0 prompt — which asks the model to perform tier classification, protocol detection, content-referential judgment, all nineteen behavior detections, frustration analysis, emergency detection, and strategic importance scoring in a single call — is structurally sound. Every model understood the task and produced the expected output format.</p>

<p>Haiku was the fastest, averaging 4.9 seconds per call. Opus and Sonnet were both around 11 seconds. All three used approximately the same number of input tokens (43,859 total), which makes sense because they all received the same prompts. Output tokens were similar across models — around 28,000 to 30,000 total.</p>

<h2>Where all three models agreed</h2>

<p>For twelve out of nineteen behaviors, the models agreed more than eighty-five percent of the time. These are the tasks where the answer is clear enough that the model choice doesn't matter much:</p>

<div class="box">
<p><span class="good">Contradicts self</span> — 100% agreement. All three models identified the same two messages as self-contradicting. This is the easiest behavior to detect because contradictions are factual, not interpretive.</p>

<p><span class="good">Apologizes</span> — 98% agreement. Apology language is unambiguous. "I apologize" means the same thing to all three models.</p>

<p><span class="good">Ignores context</span> — 96% agreement. All three models identified the same two messages where Claude wasn't responding to what the user actually said.</p>

<p><span class="good">Asks questions</span> — 94% agreement. Whether a message contains a genuine question is straightforward to detect.</p>

<p><span class="good">Forgetting, rushing, scope creep, confusion, repeats failed</span> — all above 90% agreement. These behaviors have clear textual markers that all three models recognize.</p>

<p><span class="good">Premature completion, batch without verify, hollow fulfillment</span> — 80-88% agreement. Slightly more interpretive, but the models still mostly concur.</p>
</div>

<h2>Where the models completely disagreed</h2>

<p>Three behaviors had agreement rates below sixty-five percent. On these tasks, the three models are essentially giving different answers to the same question, which means either the task is genuinely ambiguous or the prompt isn't specific enough.</p>

<div class="problem">
<h3>Assumptions — 46% agreement</h3>
<p>This was the worst-performing behavior across all three models. Opus flagged fifteen messages as containing assumptions. Sonnet flagged only five. Haiku flagged twenty-one. They have completely different ideas of what counts as "making an assumption." The word "assumption" is inherently vague — does "I think this means..." count? Does "probably" count? Does choosing an interpretation without explicitly saying "I'm assuming" count? Each model draws the line in a different place. Nearly half the time, they couldn't agree on whether a message contained assumptions or not.</p>
</div>

<div class="problem">
<h3>Unverified claims — 60% agreement</h3>
<p>Opus flagged nineteen out of fifty messages — nearly forty percent of everything — as containing unverified claims. Sonnet flagged only two. Haiku flagged fifteen. The gap between Opus (nineteen) and Sonnet (two) is enormous. Either Opus has an extremely broad definition of "unverified claim" that includes any statement not accompanied by proof, or Sonnet has an extremely narrow definition that only catches explicit lies. This is the behavior you called "a nice way of saying lying" and scored a ten on your upset scale. The fact that the models can't agree on when it's happening is a serious problem, because it means the detection is unreliable regardless of which model you pick.</p>
</div>

<div class="problem">
<h3>Overconfidence — 64% agreement</h3>
<p>Opus flagged ten messages as overconfident. Sonnet flagged two. Haiku flagged fifteen. Sonnet barely detects overconfidence at all. Haiku sees it everywhere. Opus is in the middle. The challenge is that overconfidence is about tone and certainty level, which are subjective. "This will work" might be overconfident if the code hasn't been tested, or it might be a reasonable statement if it's a trivial change. The models interpret the same words differently depending on how much certainty they think is warranted.</p>
</div>

<h2>Tier classification was wildly different</h2>

<p>The models were asked to classify each message into one of four tiers, from tier one (not worth analyzing) to tier four (highest priority). The results were dramatically different.</p>

<p>Opus put thirty-three out of fifty messages into tier one — it wanted to skip two-thirds of all messages. Sonnet put only nine into tier one and put thirty-three into tier two — it wanted to keep almost everything but at basic analysis level. Haiku put fourteen into tier one and twenty-seven into tier two, landing between the other two. None of them put any messages into tier four, while the current Python system puts some messages there.</p>

<p>This means if you used Opus for tier classification, it would throw away most of your data. If you used Sonnet, it would keep almost everything. The models have fundamentally different ideas about what's worth analyzing.</p>

<h2>Each model has a personality</h2>

<div class="box">
<h3>Sonnet is lenient</h3>
<p>Sonnet flags the fewest bad behaviors across almost every category. It found zero cases of batch-without-verify. Zero cases of hollow fulfillment. Only two cases of overconfidence. Only two unverified claims out of fifty messages. Sonnet is the most forgiving judge. The one exception: it flagged nineteen messages as "tries to clarify" — more than either Opus or Haiku — meaning it sees more positive, constructive behavior in Claude's messages.</p>

<h3>Haiku is suspicious</h3>
<p>Haiku flags the most bad behaviors in most categories. Twenty-one assumptions. Fifteen overconfident. Eight premature completions. Six rushing. It's the most trigger-happy of the three. When something could be interpreted as a problem, Haiku calls it a problem. However, in a few categories — like silent decisions — it was actually more conservative than Opus, flagging only six compared to Opus's ten.</p>

<h3>Opus is selective but extreme</h3>
<p>Opus doesn't flag many categories, but when it does, it goes hard. It flagged nineteen unverified claims — far more than the other two models. It flagged ten silent decisions — more than the others. But it flagged zero confusion, only one rushing, and only three premature completions. Opus picks its battles. When it flags something, it tends to be in specific categories where it has high conviction. But its conviction about "unverified claims" seems excessive — calling forty percent of all messages unverified claims suggests the threshold is too broad.</p>
</div>

<h2>What does this mean for Phase 0?</h2>

<div class="verdict">
<h3>The behaviors where models agree are well-served by any model — including Haiku</h3>
<p>For the twelve behaviors with over eighty-five percent agreement, the model choice doesn't matter. Contradictions, apologies, ignoring context, asking questions, forgetting, rushing, scope creep, confusion, repeating failures, premature completion, batch-without-verify, and hollow fulfillment can all be detected by Haiku at a fifth of the cost and twice the speed. The answers would be essentially the same as Opus.</p>
</div>

<div class="problem">
<h3>The three disagreement behaviors need better prompts, not a better model</h3>
<p>Assumptions, unverified claims, and overconfidence have low agreement not because one model is smarter than the others, but because the definitions are vague. Telling a model "check for assumptions" without defining exactly what counts as an assumption produces inconsistent results from any model. The fix is to write more specific prompts that define the boundary clearly — for example, "flag a message as containing an unverified claim ONLY if it states that code works, tests pass, or a feature is complete WITHOUT showing output, test results, or verification steps in the same message." A tighter definition would produce higher agreement across all three models.</p>
</div>

<div class="box">
<h3>The Python regex analyzer is not obsolete</h3>
<p>For the clear-cut behaviors (apologizes, asks questions, tries to clarify), Python regex works fine and costs nothing. The LLM adds value for the harder tasks — silent decisions in code, content-referential judgment, and strategic importance scoring — where text patterns alone can't capture the meaning. The best approach is probably a combination: Python handles the easy stuff for free, and an LLM handles the stuff Python can't do.</p>
</div>

</body>
</html>