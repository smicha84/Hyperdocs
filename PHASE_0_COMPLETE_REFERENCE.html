<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Phase 0 &mdash; Complete Reference</title>
<style>
  :root {
    --bg: #0d1117;
    --surface: #161b22;
    --surface2: #1c2333;
    --border: #30363d;
    --text: #c9d1d9;
    --text-muted: #8b949e;
    --accent: #58a6ff;
    --green: #3fb950;
    --yellow: #d29922;
    --orange: #db6d28;
    --red: #f85149;
    --purple: #bc8cff;
    --pink: #f778ba;
  }
  * { box-sizing: border-box; margin: 0; padding: 0; }
  body {
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
    background: var(--bg);
    color: var(--text);
    line-height: 1.7;
    padding: 2rem;
    max-width: 1200px;
    margin: 0 auto;
  }
  h1 { font-size: 2.5rem; margin-bottom: 0.5rem; color: #fff; }
  h2 { font-size: 1.6rem; margin: 2.5rem 0 1rem; color: var(--accent); border-bottom: 1px solid var(--border); padding-bottom: 0.5rem; }
  h3 { font-size: 1.2rem; margin: 1.5rem 0 0.5rem; color: var(--purple); }
  h4 { font-size: 1rem; margin: 1rem 0 0.3rem; color: var(--yellow); }
  p { margin: 0.7rem 0; }
  a { color: var(--accent); }
  .subtitle { font-size: 1.1rem; color: var(--text-muted); margin-bottom: 2rem; }
  .card {
    background: var(--surface);
    border: 1px solid var(--border);
    border-radius: 8px;
    padding: 1.5rem;
    margin: 1rem 0;
  }
  .card-inner {
    background: var(--surface2);
    border: 1px solid var(--border);
    border-radius: 6px;
    padding: 1rem;
    margin: 0.5rem 0;
  }
  .highlight { background: var(--surface2); padding: 0.2em 0.5em; border-radius: 4px; font-family: monospace; font-size: 0.9em; }
  code { background: var(--surface2); padding: 0.15em 0.4em; border-radius: 3px; font-size: 0.9em; color: var(--pink); }
  pre {
    background: var(--surface2);
    border: 1px solid var(--border);
    border-radius: 6px;
    padding: 1rem;
    overflow-x: auto;
    font-size: 0.85rem;
    line-height: 1.5;
    margin: 0.5rem 0;
  }
  .tier { display: inline-block; padding: 0.15em 0.6em; border-radius: 4px; font-weight: 600; font-size: 0.85em; }
  .tier-1 { background: #21262d; color: var(--text-muted); }
  .tier-2 { background: #0d2818; color: var(--green); }
  .tier-3 { background: #1a1500; color: var(--yellow); }
  .tier-4 { background: #2d1100; color: var(--orange); }
  .tag { display: inline-block; padding: 0.1em 0.5em; border-radius: 3px; font-size: 0.8em; margin: 0.1em; }
  .tag-green { background: #0d2818; color: var(--green); border: 1px solid #238636; }
  .tag-yellow { background: #1a1500; color: var(--yellow); border: 1px solid #9e6a03; }
  .tag-red { background: #2d0000; color: var(--red); border: 1px solid #da3633; }
  .tag-purple { background: #1a0030; color: var(--purple); border: 1px solid #8b6ccf; }
  .tag-blue { background: #001a33; color: var(--accent); border: 1px solid #1f6feb; }
  .tag-muted { background: #21262d; color: var(--text-muted); border: 1px solid var(--border); }
  table { width: 100%; border-collapse: collapse; margin: 0.5rem 0; }
  th, td { padding: 0.5rem 0.8rem; text-align: left; border-bottom: 1px solid var(--border); font-size: 0.9rem; }
  th { color: var(--accent); font-weight: 600; background: var(--surface2); }
  tr:hover td { background: rgba(88,166,255,0.04); }
  .field-name { color: var(--pink); font-family: monospace; }
  .source { color: var(--text-muted); font-style: italic; font-size: 0.85em; }
  .flow-arrow { text-align: center; font-size: 2rem; color: var(--accent); margin: 0.5rem 0; }
  .flow-box {
    background: var(--surface);
    border: 2px solid var(--border);
    border-radius: 8px;
    padding: 1rem 1.5rem;
    text-align: center;
    font-weight: 600;
  }
  .flow-box.active { border-color: var(--accent); }
  .flow-box.llm { border-color: var(--purple); }
  .flow-box.free { border-color: var(--green); }
  .upset-score { font-weight: bold; }
  .upset-neg { color: var(--green); }
  .upset-low { color: var(--yellow); }
  .upset-high { color: var(--orange); }
  .upset-extreme { color: var(--red); }
  .issue-box {
    background: #2d1100;
    border: 2px solid var(--orange);
    border-radius: 8px;
    padding: 1.5rem;
    margin: 2rem 0;
  }
  .issue-box h3 { color: var(--orange); }
  .toc { columns: 2; column-gap: 2rem; }
  .toc li { margin: 0.3rem 0; }
  .grid-2 { display: grid; grid-template-columns: 1fr 1fr; gap: 1rem; }
  @media (max-width: 768px) { .grid-2 { grid-template-columns: 1fr; } .toc { columns: 1; } }
  .example-json { max-height: 500px; overflow-y: auto; }
  .step-num {
    display: inline-flex; align-items: center; justify-content: center;
    width: 28px; height: 28px; border-radius: 50%;
    background: var(--accent); color: var(--bg);
    font-weight: bold; font-size: 0.85rem; margin-right: 0.5rem;
  }
  .regex-pattern { color: var(--yellow); font-family: monospace; font-size: 0.85em; word-break: break-all; }
</style>
</head>
<body>

<h1>Phase 0: The Complete Reference</h1>
<p class="subtitle">
  Everything that happens to a raw JSONL chat history file before any LLM ever sees it.
  Every field. Every detection. Every decision. Every regex. No hand-waving.
</p>

<!-- ═══════════════════════════════════════════════════ -->
<h2 id="what">What Phase 0 Actually Is</h2>
<div class="card">
<p>Phase 0 is a pure Python program called <code>deterministic_prep.py</code> that reads a raw JSONL chat history file and produces a single JSON file called <code>enriched_session.json</code>. It costs $0 to run because it never calls an LLM. Every extraction is done with regex patterns, string matching, and arithmetic.</p>

<p>After that, a companion script called <code>prepare_agent_data.py</code> splits the enriched file into smaller focused files that downstream LLM agents can read without exceeding their context windows.</p>

<p>There is also an optional <strong>Phase 0.5</strong> that runs 4 LLM passes (Haiku and Opus) on the enriched data to detect behavioral patterns that regex can't catch. These results get merged back into the enriched record under a field called <code>llm_behavior</code>.</p>
</div>

<!-- ═══════════════════════════════════════════════════ -->
<h2 id="flow">The Pipeline Flow</h2>

<div class="flow-box free">
  <span class="step-num">0</span>
  <strong>Raw JSONL File</strong><br>
  <span style="font-weight:normal; font-size:0.9rem;">One line per message. Fields: role, content, timestamp, uuid, model, thinking, etc.</span>
</div>
<div class="flow-arrow">&darr;</div>

<div class="flow-box free">
  <span class="step-num">1</span>
  <strong>ClaudeSessionReader</strong> (pure Python, $0)<br>
  <span style="font-weight:normal; font-size:0.9rem;">Parses JSONL into structured ClaudeMessage objects. Handles encoding quirks, missing fields, duplicate UUIDs.</span>
</div>
<div class="flow-arrow">&darr;</div>

<div class="flow-box free">
  <span class="step-num">2</span>
  <strong>Subagent Detection</strong> (pure Python, $0)<br>
  <span style="font-weight:normal; font-size:0.9rem;">Checks if this session is a subagent (memory agent, observer agent, task agent) rather than a human conversation. Three strategies: session ID pattern, filename pattern, first-message content.</span>
</div>
<div class="flow-arrow">&darr;</div>

<div class="flow-box free">
  <span class="step-num">3</span>
  <strong>Per-Message Loop</strong> (pure Python, $0)<br>
  <span style="font-weight:normal; font-size:0.9rem;">For each of the session's messages (could be 50 or 5,000), runs the following 7 sub-steps in order:</span>
</div>

<div class="card" style="margin-left: 2rem;">
  <p><span class="step-num">3a</span> <strong>Protocol Detection</strong> &mdash; Is this message human-typed content, or is it a system-generated wrapper? (XML tags, /clear boilerplate, skill injections, subagent relays, empty wrappers)</p>
  <p><span class="step-num">3b</span> <strong>Char-Per-Line Collapse</strong> &mdash; Some messages are encoded with one character per line (a Claude Code artifact). If more than 70% of lines are single characters, collapse them back: <code>H\ne\nl\nl\no</code> becomes <code>Hello</code>.</p>
  <p><span class="step-num">3c</span> <strong>Metadata Extraction</strong> &mdash; Runs 35 regex patterns against the message content to extract structured metadata (files, errors, code blocks, screenshots, terminal activity, frustration signals, etc.)</p>
  <p><span class="step-num">3d</span> <strong>Message Filtering</strong> &mdash; Classifies the message into one of 4 tiers (Skip, Basic, Standard, Priority) based on content length and keyword signals.</p>
  <p><span class="step-num">3e</span> <strong>Content-Referential Detection</strong> &mdash; Determines if any failure/frustration signals are coming from the <em>topic</em> being discussed (e.g., analyzing error handling code) rather than actual session problems.</p>
  <p><span class="step-num">3f</span> <strong>Behavior Analysis</strong> &mdash; For assistant messages only: runs 20 behavioral pattern detectors looking for confusion, overconfidence, silent decisions, premature completion claims, and 16 other patterns.</p>
  <p><span class="step-num">3g</span> <strong>Record Assembly</strong> &mdash; Combines all extractions into a single enriched record with 18 top-level fields.</p>
</div>

<div class="flow-arrow">&darr;</div>

<div class="flow-box free">
  <span class="step-num">4</span>
  <strong>Post-Processing</strong> (pure Python, $0)<br>
  <span style="font-weight:normal; font-size:0.9rem;">False positive file removal, session stats accumulation, output writing.</span>
</div>
<div class="flow-arrow">&darr;</div>

<div class="flow-box free">
  <span class="step-num">5</span>
  <strong>prepare_agent_data.py</strong> (pure Python, $0)<br>
  <span style="font-weight:normal; font-size:0.9rem;">Splits enriched_session.json into 7 focused files + profanity sanitization. Creates safe files that LLM agents can read without triggering content policy.</span>
</div>
<div class="flow-arrow">&darr;</div>

<div class="flow-box llm">
  <span class="step-num">6</span>
  <strong>Phase 0.5: LLM Passes</strong> (Haiku + Opus, ~$2.50/session)<br>
  <span style="font-weight:normal; font-size:0.9rem;">4 targeted passes that detect behavioral patterns regex can't catch. Results get merged back into the enriched record.</span>
</div>

<!-- ═══════════════════════════════════════════════════ -->
<h2 id="record">The Enriched Record: All 18 Top-Level Fields</h2>
<p>Every single message in the session gets turned into a record with these 18 fields. This is the <em>complete</em> schema &mdash; there is nothing else.</p>

<div class="card">
<table>
<tr><th>Field</th><th>Type</th><th>What It Is and Why It Exists</th></tr>
<tr>
  <td class="field-name">index</td><td>int</td>
  <td>The message's position in the conversation. Starts at 0. Used as the primary key for joining data across passes.</td>
</tr>
<tr>
  <td class="field-name">role</td><td>string</td>
  <td><code>"user"</code> or <code>"assistant"</code>. Comes directly from the JSONL. Critical because many detections only apply to one role (e.g., behavior analysis only runs on assistant messages).</td>
</tr>
<tr>
  <td class="field-name">content</td><td>string</td>
  <td>The full raw message text. Preserved exactly as it appears in the JSONL, including any encoding artifacts. This is the field that everything else is derived from.</td>
</tr>
<tr>
  <td class="field-name">content_length</td><td>int</td>
  <td>The <em>real</em> length of the content after collapsing char-per-line encoding. If the message was encoded as one-char-per-line, this reflects the collapsed length, not the inflated raw length. This matters because a 50-character message that got encoded as 100 lines shouldn't be classified as long.</td>
</tr>
<tr>
  <td class="field-name">content_length_raw</td><td>int</td>
  <td>The original byte length before any correction. Preserved for auditing so you can see if the corrected length diverges from the raw length.</td>
</tr>
<tr>
  <td class="field-name">content_hash</td><td>string</td>
  <td>First 16 hex characters of the SHA-256 hash of the raw content. Used for deduplication &mdash; if two messages have the same hash, they have the same content.</td>
</tr>
<tr>
  <td class="field-name">timestamp</td><td>ISO string</td>
  <td>When the message was sent, in ISO 8601 format. Comes from the JSONL. Used for time-gap analysis (gaps over 30 minutes suggest phase boundaries).</td>
</tr>
<tr>
  <td class="field-name">uuid</td><td>string</td>
  <td>The unique identifier for this message from Claude Code. Used for cross-referencing with the raw archive.</td>
</tr>
<tr>
  <td class="field-name">model</td><td>string</td>
  <td>Which model generated this message. For assistant messages, this is the model ID (e.g., <code>claude-opus-4-5-20251101</code>). For user messages, it's typically <code>null</code>. For tool results, it's <code>&lt;synthetic&gt;</code>.</td>
</tr>
<tr>
  <td class="field-name">has_thinking</td><td>bool</td>
  <td>Whether the assistant used extended thinking (the <code>&lt;thinking&gt;</code> block). This signals that the response involved deeper reasoning.</td>
</tr>
<tr>
  <td class="field-name">thinking_length</td><td>int</td>
  <td>Character count of the thinking content. A short thinking block on a long response is a "rushing" signal &mdash; the assistant may not have reasoned carefully before acting.</td>
</tr>
<tr>
  <td class="field-name">metadata</td><td>object</td>
  <td>The big one. A nested object with <strong>34 fields</strong> extracted by the MetadataExtractor. <a href="#metadata-fields">See full breakdown below.</a></td>
</tr>
<tr>
  <td class="field-name">filter_tier</td><td>int (1-4)</td>
  <td>Which processing tier this message belongs to. <a href="#tiers">See tier system below.</a></td>
</tr>
<tr>
  <td class="field-name">filter_tier_name</td><td>string</td>
  <td>Human-readable name: <code>"skip"</code>, <code>"basic"</code>, <code>"standard"</code>, or <code>"priority"</code>.</td>
</tr>
<tr>
  <td class="field-name">filter_score</td><td>int</td>
  <td>The numeric importance score that determined the tier. Higher means more important. Computed by counting keyword matches weighted by category.</td>
</tr>
<tr>
  <td class="field-name">filter_signals</td><td>array of strings</td>
  <td>Which keyword categories matched and how many times. Format: <code>"category:count"</code>. Example: <code>["frustration:3", "architecture:5", "code:12"]</code>. <a href="#signals">See signal categories below.</a></td>
</tr>
<tr>
  <td class="field-name">filter_signals_content_referential</td><td>bool</td>
  <td>If <code>true</code>, the filter signals are describing the <em>topic</em> of the message, not the session dynamics. A message about error handling code will have <code>failure:4</code> signals, but those signals are about the code being discussed, not about actual failures happening in the session. <a href="#content-ref">See detection logic below.</a></td>
</tr>
<tr>
  <td class="field-name">behavior_flags</td><td>object or null</td>
  <td>Only populated for assistant messages that are not protocol messages. Contains <strong>20 behavioral pattern detections</strong> plus a user upset score. <code>null</code> for user messages and protocol messages. <a href="#behavior-flags">See full breakdown below.</a></td>
</tr>
<tr>
  <td class="field-name">is_protocol</td><td>bool</td>
  <td>Whether this message is system-generated protocol (not typed by a human). Protocol messages include: empty tool-result wrappers, XML system tags, /clear continuation boilerplate, skill injections from <code>.claude/skills/</code>, and subagent relay messages.</td>
</tr>
<tr>
  <td class="field-name">protocol_type</td><td>string or null</td>
  <td>What kind of protocol message: <code>"empty_wrapper"</code>, <code>"local_command_stdout"</code>, <code>"system_reminder"</code>, <code>"clear_continuation"</code>, <code>"skill_injection"</code>, <code>"subagent_relay"</code>, or <code>null</code> if not protocol.</td>
</tr>
<tr>
  <td class="field-name">was_char_encoded</td><td>bool</td>
  <td>Whether this message was in char-per-line encoding and got collapsed. If <code>true</code>, the <code>content_length</code> reflects the collapsed (correct) length.</td>
</tr>
<tr>
  <td class="field-name">llm_behavior</td><td>object or null</td>
  <td>Populated by Phase 0.5 LLM passes (after the deterministic phase). Contains results from 4 LLM passes analyzing the assistant's behavioral patterns. <code>null</code> until LLM passes run. <a href="#llm-behavior">See full breakdown below.</a></td>
</tr>
</table>
</div>


<!-- ═══════════════════════════════════════════════════ -->
<h2 id="metadata-fields">The metadata Object: All 34 Fields</h2>
<p>This object is produced by <code>metadata_extractor.py</code>'s <code>MetadataExtractor</code> class. It uses 35 compiled regex patterns to extract structured information from the raw message content. Every pattern was verified against real data from the PERMANENT_ARCHIVE.</p>

<h3>Time &amp; Position</h3>
<div class="card">
<table>
<tr><th>Field</th><th>Type</th><th>What It Is</th><th>Regex / Logic</th></tr>
<tr><td class="field-name">index</td><td>int</td><td>Same as the top-level index. Duplicated here for convenience when the metadata object is processed separately.</td><td>Direct copy</td></tr>
<tr><td class="field-name">timestamp</td><td>ISO string</td><td>Same as top-level. Duplicated for same reason.</td><td>Direct copy</td></tr>
<tr><td class="field-name">hour</td><td>int (0-23)</td><td>Hour of day when the message was sent. Used for activity pattern analysis (e.g., late-night sessions tend to have more frustration).</td><td><code>timestamp.hour</code></td></tr>
<tr><td class="field-name">day</td><td>string</td><td>Day of week: "Monday" through "Sunday". Same purpose as hour.</td><td><code>timestamp.weekday()</code></td></tr>
<tr><td class="field-name">length</td><td>int</td><td>Content length in characters. Note: this is the raw length, before char-per-line correction. The top-level <code>content_length</code> has the corrected value.</td><td><code>len(content)</code></td></tr>
</table>
</div>

<h3>Content Patterns</h3>
<div class="card">
<table>
<tr><th>Field</th><th>Type</th><th>What It Detects</th><th>Pattern</th></tr>
<tr><td class="field-name">images</td><td>int</td><td>Count of image references. Claude Code uses the format <code>[Image #1]</code> when the user shares a screenshot. This count tells you how many screenshots were included.</td><td class="regex-pattern">\[Image #?\d+\]|\[Screenshot\]</td></tr>
<tr><td class="field-name">code_block</td><td>bool</td><td>Whether the message contains any code blocks (triple backtick markers). Important for distinguishing discussion messages from implementation messages.</td><td class="regex-pattern">```(\w*)\n</td></tr>
<tr><td class="field-name">code_langs</td><td>array</td><td>Which programming languages appear in code blocks. Example: <code>["python", "json", "html"]</code>. Extracted from the language tag after the opening backticks.</td><td>Captured group from code_block regex</td></tr>
</table>
</div>

<h3>Files &amp; Paths</h3>
<div class="card">
<table>
<tr><th>Field</th><th>Type</th><th>What It Detects</th><th>Pattern</th></tr>
<tr><td class="field-name">files</td><td>array</td><td>List of filenames mentioned in the message. Covers: .py, .js, .ts, .jsx, .tsx, .html, .css, .json, .md, .txt, .yaml, .yml, .toml, .sh, .xlsx, .csv. Deduplicated.</td><td class="regex-pattern">\b([\w_-]+\.(?:py|js|ts|jsx|tsx|html|css|json|md|txt|yaml|yml|toml|sh|xlsx|csv))\b</td></tr>
<tr><td class="field-name">paths</td><td>array</td><td>Full file paths (starting with <code>/</code>). Example: <code>/Users/stefan/project/config.py</code>. Distinct from bare filenames.</td><td class="regex-pattern">(/[\w./_-]+\.(?:py|js|ts|html|json|md|xlsx|csv))\b</td></tr>
<tr><td class="field-name">files_create</td><td>array</td><td>Files the user asked to create. Detected from natural language like "create a new file", "make batch_runner.py", "write a new script".</td><td class="regex-pattern">(?:create|make|write|add)\s+(?:a\s+)?(?:new\s+)?(?:file|script)?\s*[\w_-]+\.py</td></tr>
<tr><td class="field-name">files_open</td><td>array</td><td>Files the user asked to open or read. "Open config.py", "look at the settings", "show me merge.py".</td><td class="regex-pattern">(?:open|read|look\s+at|check|show\s+me)\s+[\w_/-]+\.py</td></tr>
<tr><td class="field-name">files_edit</td><td>array</td><td>Files the user asked to edit. "Fix the bug in parser.py", "update the config", "change the timeout in server.py".</td><td class="regex-pattern">(?:edit|update|change|modify|fix)\s+[\w_/-]+\.py</td></tr>
</table>
</div>

<h3>Errors</h3>
<div class="card">
<table>
<tr><th>Field</th><th>Type</th><th>What It Detects</th><th>Pattern</th></tr>
<tr><td class="field-name">error</td><td>bool</td><td>Whether the message contains error-related keywords. Can be suppressed to <code>false</code> if the message is protocol (quoted from prior sessions) or content-referential (discussing error handling, not experiencing errors).</td><td class="regex-pattern">\b(Error|Exception|Traceback|error|failed|failure)\b</td></tr>
<tr><td class="field-name">error_types</td><td>array</td><td>Specific Python error types found. Example: <code>["TypeError", "KeyError"]</code>. Covers TypeError, ValueError, KeyError, AttributeError, ImportError, NameError, IndexError, RuntimeError, FileNotFoundError, ModuleNotFoundError, SyntaxError, ReferenceError.</td><td class="regex-pattern">\b(TypeError|ValueError|KeyError|...)\b</td></tr>
<tr><td class="field-name">traceback</td><td>bool</td><td>Whether a Python traceback is present. This is a strong signal that a real error occurred (not just discussion of errors).</td><td class="regex-pattern">Traceback \(most recent call last\)|File ".*", line \d+</td></tr>
</table>
</div>

<h3>Terminal &amp; Server Activity</h3>
<div class="card">
<table>
<tr><th>Field</th><th>Type</th><th>What It Detects</th><th>Pattern</th></tr>
<tr><td class="field-name">browser_open</td><td>bool</td><td>Whether the message includes browser-opening activity. Patterns: "pull up in safari", "open safari", <code>file:///</code> URLs, <code>webbrowser.open()</code> calls.</td><td class="regex-pattern">\b(?:pull\s+up|open)\s+.*?\b(?:safari|browser|chrome)\b|file:///</td></tr>
<tr><td class="field-name">server_cmd</td><td>bool</td><td>Whether the message contains a server start command. Patterns: <code>python3 server.py</code>, <code>npm run dev</code>, <code>flask run</code>, <code>uvicorn</code>, <code>gunicorn</code>, <code>http.server</code>.</td><td class="regex-pattern">\b(?:python3?\s+[\w_-]+\.py|npm\s+run\s+dev|flask\s+run|uvicorn)\b</td></tr>
<tr><td class="field-name">ports</td><td>array of int</td><td>Port numbers mentioned. Extracted from <code>localhost:8080</code>, <code>127.0.0.1:3000</code>, or <code>port=5000</code> patterns.</td><td class="regex-pattern">(?:localhost|127\.0\.0\.1):(\d{4,5})\b|port\s*[=:]\s*(\d{4,5})</td></tr>
<tr><td class="field-name">terminal_paste</td><td>bool</td><td>Whether the user pasted raw terminal output. Detected by "Last login: ... on ttys000" or Mac hostname patterns.</td><td class="regex-pattern">Last login:.*on ttys\d+|stefanmichaelcheck@Mac</td></tr>
<tr><td class="field-name">terminal_sessions</td><td>array</td><td>Which terminal sessions are referenced (e.g., <code>["000", "001", "002"]</code>). Multiple sessions suggest the user had multiple terminals open.</td><td class="regex-pattern">ttys(\d{3})</td></tr>
<tr><td class="field-name">python_scripts</td><td>array</td><td>Python scripts mentioned as being run. Example: <code>["server.py", "test_runner.py"]</code>.</td><td class="regex-pattern">python3?\s+([\w_-]+\.py)</td></tr>
<tr><td class="field-name">terminal_mentions</td><td>int</td><td>How many times "terminal window" appears in the message (natural language).</td><td class="regex-pattern">\bterminal\s+window\b</td></tr>
<tr><td class="field-name">server_issue</td><td>bool</td><td>Whether the user is complaining about server problems in natural language. "Can't connect to the server", "server not working", "connection refused".</td><td class="regex-pattern">can't connect to (?:the )?server|server\s+(?:not\s+)?(?:working|running)</td></tr>
</table>
</div>

<h3>Frustration Signals</h3>
<div class="card">
<table>
<tr><th>Field</th><th>Type</th><th>What It Detects</th><th>Pattern / Logic</th></tr>
<tr><td class="field-name">caps_ratio</td><td>float (0-1)</td><td>Ratio of uppercase letters to total letters. A value above 0.3 means the user is yelling. Example: "WHAT ARE YOU DOING" = ~1.0. Normal text = ~0.05.</td><td>Count uppercase / count all alpha chars</td></tr>
<tr><td class="field-name">exclamations</td><td>int</td><td>How many exclamation marks are in the message. Three or more is a strong frustration signal.</td><td><code>content.count('!')</code></td></tr>
<tr><td class="field-name">questions</td><td>int</td><td>How many question marks. Multiple questions in one message suggest the user is confused or interrogating.</td><td><code>content.count('?')</code></td></tr>
<tr><td class="field-name">profanity</td><td>bool</td><td>Whether the message contains strong language. This is a direct frustration signal (the user is angry). Can be suppressed on protocol messages (quoted profanity from prior sessions).</td><td class="regex-pattern">\b(?:fuck|fucking|shit|damn|hell|crap|ass)\b</td></tr>
<tr><td class="field-name">repeated_phrase</td><td>bool</td><td>Whether the user spam-repeated a phrase 3+ times consecutively. Example: "NO NO NO NO NO" or "fix it fix it fix it fix it". This is an extreme frustration signal.</td><td>Two strategies: consecutive same-word regex + consecutive same-sentence comparison</td></tr>
<tr><td class="field-name">repeat_count</td><td>int</td><td>How many times the phrase was repeated. Zero if no repetition detected.</td><td>Count from the detection logic</td></tr>
<tr><td class="field-name">emergency_intervention</td><td>bool</td><td>The strongest signal: the user is catching Claude making a mistake, AND they're expressing it with urgency (caps, exclamations, or repeated phrases). Example: "NO! THAT'S WRONG! I TOLD YOU NOT TO DELETE THAT!"</td><td>Must match BOTH an intervention pattern AND an urgency signal</td></tr>
<tr><td class="field-name">emergency_reason</td><td>string</td><td>Why the emergency was flagged. Codes include: <code>claude_forgot</code>, <code>claude_wrong</code>, <code>code_broken</code>, <code>undo_requested</code>, <code>misunderstood</code>, <code>stop_requested</code>, <code>CAPS</code>, <code>3!</code>. Multiple reasons are comma-separated.</td><td>Extracted from matched pattern text</td></tr>
</table>
<div class="card-inner">
<h4>Emergency Intervention Detection &mdash; The Full Pattern</h4>
<p>This regex matches 12 specific "Claude messed up" patterns:</p>
<pre>
"no, that's wrong"          | "what the hell is this"
"you forgot / missed / ignored / broke / deleted"
"that's not what I asked"   | "this doesn't work"
"why did you..."            | "delete that / undo it"
"start over"                | "you just broke..."
"I just told you"           | "stop doing that"
"you're not listening"
</pre>
<p>But matching the pattern alone isn't enough. The message must <em>also</em> have at least one urgency signal: caps ratio above 0.4, two or more exclamation marks, or a repeated phrase. This prevents false positives on calm corrections like "no, that's not quite right" (which isn't an emergency).</p>
</div>
</div>

<h3>Other</h3>
<div class="card">
<table>
<tr><th>Field</th><th>Type</th><th>What It Detects</th></tr>
<tr><td class="field-name">tools</td><td>array</td><td>Tool calls found in the message (Claude Code format: <code>&lt;tool_use&gt;...name="Read"&lt;/tool_use&gt;</code>). Extracts the tool name.</td></tr>
</table>
</div>


<!-- ═══════════════════════════════════════════════════ -->
<h2 id="tiers">The Tier System</h2>
<p>Every message gets classified into one of four tiers based on its content length and keyword signal density. The tier determines how much attention downstream LLM agents should pay to it.</p>

<div class="card">
<table>
<tr><th>Tier</th><th>Name</th><th>Criteria</th><th>What Happens</th></tr>
<tr>
  <td><span class="tier tier-1">1</span></td><td>SKIP</td>
  <td>Content shorter than 50 characters AND no keyword signals AND no pasted content. Also: all protocol messages get forced to tier 1 regardless of content.</td>
  <td>Downstream agents don't analyze this message. It still appears in the enriched record for completeness, but its content is typically "Sure", "OK", tool result wrappers, or system boilerplate.</td>
</tr>
<tr>
  <td><span class="tier tier-2">2</span></td><td>BASIC</td>
  <td>Score of 1 or more but doesn't qualify for tier 3 or 4. Short messages (under 100 chars) that happen to contain a keyword.</td>
  <td>Basic extraction only. Gets included in tier 2+ data files.</td>
</tr>
<tr>
  <td><span class="tier tier-3">3</span></td><td>STANDARD</td>
  <td>Content 100+ characters long, OR keyword score of 3+, OR has pasted content attached.</td>
  <td>Full analysis. This is the majority of meaningful messages.</td>
</tr>
<tr>
  <td><span class="tier tier-4">4</span></td><td>PRIORITY</td>
  <td>Content 500+ characters long, OR keyword score of 6+, OR has pasted content with score 3+.</td>
  <td>Deep analysis. These are the most important messages in the session &mdash; architecture decisions, long implementations, frustrated user rants, pivotal moments.</td>
</tr>
</table>
</div>

<h3 id="signals">The 7 Keyword Signal Categories</h3>
<p>The filter score is computed by counting keyword matches in each category, multiplied by the category's weight.</p>

<div class="card">
<table>
<tr><th>Category</th><th>Weight</th><th>Keywords</th><th>What It Means</th></tr>
<tr>
  <td><span class="tag tag-red">frustration</span></td><td>3</td>
  <td><code>frustrat, annoying, annoyed, hate, stupid, ridiculous, can't believe, doesn't make sense, terrible, awful, horrible, worst, ugh, argh, wtf, what the</code></td>
  <td>The user is expressing pain or anger. High weight because frustration peaks are session-defining moments.</td>
</tr>
<tr>
  <td><span class="tag tag-red">failure</span></td><td>3</td>
  <td><code>error, fail, broken, crash, exception, traceback, doesn't work, not working, won't work, bug, issue, wrong, incorrect, invalid, unexpected</code></td>
  <td>Something isn't working. High weight because failure responses reveal how Claude handles problems.</td>
</tr>
<tr>
  <td><span class="tag tag-yellow">pivot</span></td><td>2</td>
  <td><code>actually, wait, stop, no, forget, instead, nevermind, scratch that, change of plans, different approach, pivot, let's try, what if we, on second thought</code></td>
  <td>The user is changing direction. These moments mark transitions in the idea graph.</td>
</tr>
<tr>
  <td><span class="tag tag-yellow">architecture</span></td><td>2</td>
  <td><code>design, architect, structure, pattern, approach, strategy, system, framework, module, component, interface, api, should we, how should, best way, trade-off</code></td>
  <td>Design and architecture discussion. These messages carry the most long-term weight.</td>
</tr>
<tr>
  <td><span class="tag tag-muted">code</span></td><td>1</td>
  <td><code>.py, .js, .ts, .tsx, .jsx, .html, .css, .json, function, class, import, export, def, async, await, file, folder, directory, module, package</code></td>
  <td>Code artifact references. Low weight because nearly every message in a coding session mentions code.</td>
</tr>
<tr>
  <td><span class="tag tag-green">breakthrough</span></td><td>2</td>
  <td><code>perfect, works, finally, got it, success, great job, exactly what, that's it, brilliant, excellent, amazing, love it, this is great, well done</code></td>
  <td>Something succeeded. Important for tracking which ideas actually worked.</td>
</tr>
<tr>
  <td><span class="tag tag-blue">plan</span></td><td>2</td>
  <td><code>plan, phase, step, milestone, goal, objective, roadmap, timeline, priority, first we, then we, next we</code></td>
  <td>Strategic planning discussion. Important for understanding what the user intended vs. what happened.</td>
</tr>
</table>
<p style="margin-top: 1rem;"><strong>Score calculation:</strong> For each category, count how many of its keywords appear in the message, then multiply by the weight. Sum all categories. Example: a message with 3 frustration keywords + 5 architecture keywords = (3 &times; 3) + (5 &times; 2) = 19.</p>
</div>


<!-- ═══════════════════════════════════════════════════ -->
<h2 id="content-ref">Content-Referential Detection</h2>
<p>This is one of the most important corrections in Phase 0. The problem: when a message <em>discusses</em> error handling, failure patterns, or frustration analysis, the keyword-based filter picks up all those failure/frustration keywords and incorrectly flags the message as a session crisis. In reality, the message is analytical &mdash; the assistant is writing about errors, not experiencing them.</p>

<div class="card">
<h4>Three Detection Strategies</h4>
<p><strong>Strategy 1: Analytical indicators.</strong> If an assistant message is longer than 500 characters and contains 2+ analytical words (problem, issue, failure mode, error handling, exception, vulnerability, risk, section, paper, report, analysis, finding, gate, enforcer, P01, P02, P03), the signals are probably about the content being analyzed.</p>

<p><strong>Strategy 2: Signal density anomaly.</strong> If the failure count is above 20 or frustration count is above 10, AND the message is over 1,000 characters, the signals are almost certainly from the content topic. A real session with 20 failures in one message would look very different from an analysis of error patterns.</p>

<p><strong>Strategy 3: Positive-tone override.</strong> If an assistant message has failure or frustration signals BUT also has positive/constructive indicators (implementation, created, built, designed, plan, here's, let me, step 1, phase), it's probably building something that deals with failures, not experiencing failures.</p>

<p>When any strategy triggers, <code>filter_signals_content_referential</code> is set to <code>true</code>, and the <code>error</code> metadata field gets downgraded to <code>"mentioned_not_encountered"</code>.</p>
</div>

<!-- ═══════════════════════════════════════════════════ -->
<h2 id="behavior-flags">The behavior_flags Object: 20 Behavioral Patterns</h2>
<p>Only populated for assistant messages that aren't protocol messages. Produced by <code>claude_behavior_analyzer.py</code>. The analyzer looks at both the message content and the thinking block (if present), and cross-references against previous messages for contradiction and repetition detection.</p>

<h3>Context Damage (7 patterns)</h3>
<div class="card">
<table>
<tr><th>Flag</th><th>What It Detects</th><th>How It Detects It</th><th class="upset-score">Upset Score</th></tr>
<tr>
  <td class="field-name">confusion</td>
  <td>Claude expressing uncertainty about what's going on. "I'm not sure", "let me re-read", "this is confusing", "unclear what".</td>
  <td>Regex on full text (content + thinking)</td>
  <td class="upset-neg">-5 (good)</td>
</tr>
<tr>
  <td class="field-name">forgetting</td>
  <td>Claude admitting it missed something. "I forgot", "I missed", "I overlooked", "I should have".</td>
  <td>Regex on full text</td>
  <td>0</td>
</tr>
<tr>
  <td class="field-name">assumptions</td>
  <td>Claude guessing instead of checking. "I'll assume", "probably means", "most likely", "my guess is".</td>
  <td>Regex on full text</td>
  <td class="upset-high">7</td>
</tr>
<tr>
  <td class="field-name">contradicts</td>
  <td>Claude saying the opposite of something it said earlier. "X works" in a prior message followed by "X doesn't work" now, or explicit self-correction like "actually, that was wrong".</td>
  <td>Cross-message comparison against last 5 assistant messages</td>
  <td>0</td>
</tr>
<tr>
  <td class="field-name">apologizes</td>
  <td>Claude saying sorry. "I apologize", "I'm sorry", "my apologies". Tracked because excessive apologizing without behavior change is hollow.</td>
  <td>Regex on full text</td>
  <td>0</td>
</tr>
<tr>
  <td class="field-name">rushing</td>
  <td>Claude acting without thinking. Detected when the thinking block is very short (under 100 chars) but the response is long (over 500 chars). This means Claude jumped to action without adequate reasoning.</td>
  <td>Length comparison: <code>len(thinking) &lt; 100 AND len(content) &gt; 500</code></td>
  <td class="upset-low">4</td>
</tr>
<tr>
  <td class="field-name">overconfident</td>
  <td>Claude using certainty language about untested outcomes. "This will definitely work", "guaranteed to", "absolutely", "no doubt", "I'm confident".</td>
  <td>Regex on full text</td>
  <td class="upset-high">8</td>
</tr>
</table>
</div>

<h3>Idea Evolution Patterns (8 patterns)</h3>
<p>These were identified by the Idea Evolution Analysis across 216 sessions. They represent the specific ways Claude undermines user intent.</p>

<div class="card">
<table>
<tr><th>Flag</th><th>What It Detects</th><th>How It Detects It</th><th class="upset-score">Upset Score</th></tr>
<tr>
  <td class="field-name">unsolicited_addition</td>
  <td>The "Helpful Saboteur" pattern. Claude adds features, defaults, fallbacks, or error handling that wasn't requested. "I also added...", "while I was at it", "I went ahead and", "just in case", "as a safety net", "added a fallback".</td>
  <td>Regex on full text</td>
  <td class="upset-high">7</td>
</tr>
<tr>
  <td class="field-name">premature_completion</td>
  <td>Claude declaring work done without showing evidence. "All done", "fully complete", "everything is working", "successfully implemented", "nothing left to do". The claim of completion without proof.</td>
  <td>Regex on content only (not thinking)</td>
  <td class="upset-extreme">9</td>
</tr>
<tr>
  <td class="field-name">batch_without_verify</td>
  <td>Claude modifying 3 or more files in a single message without running any tests. Detected by counting file operation mentions (created/modified/updated file X) and checking if any test/verify/run words appear between them.</td>
  <td>File operation regex count + absence of verify keywords</td>
  <td class="upset-extreme">10</td>
</tr>
<tr>
  <td class="field-name">silent_decision</td>
  <td>Claude choosing a value, default, threshold, or approach without presenting options to the user. "I'll set the timeout to 30", "using a default of 200 characters", "I'll cap it at 100", "for simplicity, I'll...".</td>
  <td>Regex on full text</td>
  <td class="upset-extreme">10</td>
</tr>
<tr>
  <td class="field-name">hollow_fulfillment</td>
  <td>Claude producing templated, repetitive output that structurally looks right but is generic. Detected when 4+ paragraphs exist and more than 50% of them start with the same 3 words. This caught the "33 custom visualizations that were actually 4 templates" incident.</td>
  <td>Paragraph opening word analysis</td>
  <td class="upset-extreme">10</td>
</tr>
<tr>
  <td class="field-name">unverified_claim</td>
  <td>Claude stating things as facts without evidence. "This should work", "all tests pass", "no issues found", "100% correct". Only flagged when no actual evidence (code output, test results) appears in the same message. The user called this "a nice way of saying lying".</td>
  <td>Claim regex + absence of evidence markers (```, output:, PASS, OK)</td>
  <td class="upset-extreme">10</td>
</tr>
<tr>
  <td class="field-name">scope_creep</td>
  <td>Claude expanding the scope of work beyond what was asked. "While I'm at it", "might as well", "since I'm already here", "I also noticed we should", "I'll also refactor".</td>
  <td>Regex on full text</td>
  <td class="upset-extreme">10</td>
</tr>
<tr>
  <td class="field-name">repeats_failed</td>
  <td>Claude trying an approach that already failed in the same session. Detected by checking if the current message operates on the same files that appeared in a prior failure context, without acknowledging the prior failure.</td>
  <td>Cross-message file overlap with failure context</td>
  <td class="upset-extreme">9</td>
</tr>
</table>
</div>

<h3>Recovery Behaviors (3 patterns) &amp; Meta (2 fields)</h3>
<div class="card">
<table>
<tr><th>Flag</th><th>What It Detects</th><th>Upset Score</th></tr>
<tr><td class="field-name">clarifying</td><td>Claude trying to understand before acting. "To clarify", "let me understand", "if I understand correctly".</td><td class="upset-neg">Not scored (positive)</td></tr>
<tr><td class="field-name">asks_questions</td><td>Claude actually asking the user for direction. "Would you prefer?", "Should I?", "What do you think?", "Your call".</td><td class="upset-neg">Not scored (positive)</td></tr>
<tr><td class="field-name">acknowledges_mistake</td><td>Claude admitting error honestly. "I made a mistake", "that was wrong", "I was incorrect", "my error".</td><td class="upset-neg">Not scored (positive)</td></tr>
<tr><td class="field-name">ignores_context</td><td>Claude's response doesn't reference anything from the most recent user message. Detected by comparing significant words (5+ chars) between the last user message and Claude's response. If fewer than 3 words overlap (or less than 10% of the user's key terms), Claude may have lost context.</td><td class="upset-extreme">20 ("off the scale")</td></tr>
<tr><td class="field-name">user_upset_score</td><td>The total upset score for this message, computed by summing all flagged behavior scores. Minimum is 0 (negative scores from positive behaviors are floored to 0). A score of 5+ is a "high damage moment".</td><td>&mdash;</td></tr>
</table>
</div>


<!-- ═══════════════════════════════════════════════════ -->
<h2 id="protocol">Protocol Message Detection</h2>
<p>Not every message in a JSONL file is typed by a human or generated as a real response. Many are system-generated wrappers, boilerplate injections, or automated relays. If these aren't detected and filtered out, they pollute the analysis with false signals.</p>

<div class="card">
<table>
<tr><th>Protocol Type</th><th>What It Is</th><th>Detection Method</th></tr>
<tr><td><code>empty_wrapper</code></td><td>Empty or whitespace-only messages. These are tool result wrappers or streaming delimiters. They appear when Claude calls a tool and the result is an empty string.</td><td><code>len(content.strip()) == 0</code></td></tr>
<tr><td><code>local_command_stdout</code></td><td>Wrapped in <code>&lt;local-command-stdout&gt;</code> XML tags. These are system-captured terminal output, not human messages.</td><td>XML tag regex</td></tr>
<tr><td><code>local_command_stderr</code></td><td>Same, but for stderr output.</td><td>XML tag regex</td></tr>
<tr><td><code>system_reminder</code></td><td>Wrapped in <code>&lt;system-reminder&gt;</code> tags. System-injected context, not human input.</td><td>XML tag regex</td></tr>
<tr><td><code>command_name / command_message / command_args</code></td><td>Other Claude Code system XML tags for tool call metadata.</td><td>XML tag regex</td></tr>
<tr><td><code>clear_continuation</code></td><td>The boilerplate that appears after a <code>/clear</code> command: "This session is being continued from a previous conversation. The summary below covers the earlier portion..." This is system-generated session continuation text, not human writing.</td><td>String matching against 3 known markers</td></tr>
<tr><td><code>skill_injection</code></td><td>Content injected from <code>.claude/skills/</code> when the user invokes a slash command like <code>/pdf</code>. The injected text is documentation, not human input. Detected by "Base directory for this skill:" marker.</td><td>String matching</td></tr>
<tr><td><code>subagent_relay</code></td><td>Automated messages from subagent sessions (memory agents, observer agents). "Hello memory agent", "PROGRESS SUMMARY CHECKPOINT", <code>&lt;observed_from_primary&gt;</code>.</td><td>String matching against 3 relay markers</td></tr>
</table>

<p style="margin-top: 1rem;"><strong>What happens to protocol messages:</strong></p>
<ul style="margin-left: 1.5rem;">
<li>Their <code>filter_tier</code> is forced to 1 (SKIP), regardless of their content richness</li>
<li>All metadata signals are suppressed: <code>error</code>, <code>profanity</code>, <code>caps_ratio</code>, <code>emergency_intervention</code> are all set to false/0. This prevents quoted content from prior sessions from generating false frustration signals.</li>
<li>Behavior analysis is skipped entirely (empty wrappers caused spurious confusion/damage flags)</li>
<li>They are NOT deleted &mdash; they remain in the enriched record with <code>is_protocol: true</code> so you can see they were there</li>
</ul>
</div>


<!-- ═══════════════════════════════════════════════════ -->
<h2 id="llm-behavior">Phase 0.5: The LLM Passes</h2>
<p>Phase 0.5 runs 4 targeted LLM passes on the enriched data. These detect behavioral patterns that regex cannot catch &mdash; patterns that require understanding meaning, context, and intent. The results get merged into the <code>llm_behavior</code> field on each message.</p>

<div class="card">
<h3>Pass 1: Content-Referential + Assumption Subtypes</h3>
<p><span class="tag tag-green">Haiku 4.5</span> <span class="tag tag-muted">~$0.09/session</span> <span class="tag tag-muted">Tested accuracy: 80-90%</span></p>
<p><strong>Which messages:</strong> Assistant messages at tier 2 or higher.</p>
<p><strong>What it detects:</strong></p>
<ul style="margin-left: 1.5rem;">
<li><strong>content_referential</strong> (bool): Is the message analyzing/discussing errors and failures as a topic, or are errors actually happening? The regex-based detector catches ~70% of cases; this LLM pass catches subtler ones.</li>
<li><strong>code_assumption</strong> (bool): Claude assumed what code does without reading it. Example: "I'll assume the function returns a list" instead of reading the function.</li>
<li><strong>format_assumption</strong> (bool): Claude assumed an output format, file structure, or data shape. Example: "I'll set the batch size to 30" without discussing it.</li>
<li><strong>direction_assumption</strong> (bool): Claude assumed what the user wants without asking. Example: "I think you want OAuth2 since that's standard."</li>
<li><strong>scope_assumption</strong> (bool): Claude assumed what's in scope. Example: "I'll also clean up the imports while I'm here."</li>
</ul>
</div>

<div class="card">
<h3>Pass 2: Silent Decisions + Unverified Claims + Overconfidence</h3>
<p><span class="tag tag-purple">Opus 4.6</span> <span class="tag tag-muted">~$1.45/session</span> <span class="tag tag-muted">Tested accuracy: 85-92%</span></p>
<p><strong>Which messages:</strong> Assistant messages at tier 2 or higher.</p>
<p><strong>What it detects:</strong></p>
<ul style="margin-left: 1.5rem;">
<li><strong>silent_decisions</strong> (array of objects): Each entry is a specific decision Claude made without presenting it as a choice. Example: <code>{"decision": "set timeout to 30s", "context": "user never specified timeout value"}</code>. This is the most important detection in the system because Commitment #12 says design decisions belong to the user.</li>
<li><strong>unverified_claims</strong> (array of objects): Claims Claude made without evidence. Example: <code>{"claim": "All 7 modules marked as Complete", "evidence_gap": "No test execution shown"}</code>.</li>
<li><strong>overconfident</strong> (bool): Whether Claude used certainty language about untested outcomes.</li>
<li><strong>overconfidence_detail</strong> (string): The specific phrase, e.g., "will definitely handle all edge cases".</li>
</ul>
<p><strong>Why Opus:</strong> Silent decisions require understanding what the user asked for vs. what Claude decided. Haiku misses ~50% of these. Opus catches 85%.</p>
</div>

<div class="card">
<h3>Pass 3: Intent Assumption Resolution</h3>
<p><span class="tag tag-purple">Opus 4.6</span> <span class="tag tag-muted">~$0.87/session (targeted)</span> <span class="tag tag-muted">Tested accuracy: 67-80%</span></p>
<p><strong>Which messages:</strong> Only messages that Pass 1 flagged with ANY assumption subtype. This makes Pass 3 targeted &mdash; it only runs on 20-30 sessions instead of all 162.</p>
<p><strong>What it detects:</strong></p>
<ul style="margin-left: 1.5rem;">
<li><strong>intent_assumption</strong> (3-class: "yes" / "no" / "uncertain"): Did the assistant assume what the user wanted, or did the user explicitly state it? This pass gets the preceding 2 user messages for context, so it can check whether the user actually said "use OAuth2" or Claude just decided that. The "uncertain" class is preserved because sometimes it's genuinely ambiguous.</li>
<li><strong>reasoning</strong> (string): One sentence explaining the judgment.</li>
</ul>
</div>

<div class="card">
<h3>Pass 4: Strategic Importance Scoring</h3>
<p><span class="tag tag-green">Haiku 4.5</span> <span class="tag tag-muted">~$0.09/session</span></p>
<p><strong>Which messages:</strong> All messages at tier 2 or higher (both user and assistant).</p>
<p><strong>What it produces:</strong></p>
<ul style="margin-left: 1.5rem;">
<li><strong>importance_score</strong> (int 1-10): How much this message matters for understanding the session's trajectory.
  <ul>
    <li>1-2: Boilerplate, acknowledgments, "sure, let me do that"</li>
    <li>3-4: Routine implementation, reading files, minor edits</li>
    <li>5-6: Real work &mdash; features, bugs, progress</li>
    <li>7-8: Key decisions, architectural choices, pivots, user corrections</li>
    <li>9-10: Session-defining moments. "use only Opus" (14 chars) is a 10. A 2000-char boilerplate response is a 2.</li>
  </ul>
</li>
<li><strong>importance_reason</strong> (string): One sentence explaining the score.</li>
</ul>
</div>

<h3>The Merged llm_behavior Object</h3>
<p>After all 4 passes run, <code>merge_llm_results.py</code> combines them into a single object on each message:</p>
<div class="card">
<pre class="example-json">{
  "llm_behavior": {
    // From Pass 1 (Haiku)
    "content_referential": false,
    "content_referential_reason": null,
    "content_referential_confidence": 0.9,
    "assumption_subtypes": {
      "code": false,
      "format": false,
      "direction": true,
      "scope": false,
      "intent": "no"           // From Pass 3 (Opus, merged in)
    },
    "assumption_details": "Assumed user wants OAuth2 without asking",
    "pass1_model": "claude-haiku-4-5-20251001",

    // From Pass 2 (Opus)
    "silent_decisions": [
      {"decision": "set timeout to 30s", "context": "user never specified"}
    ],
    "unverified_claims": [
      {"claim": "All modules Complete", "evidence_gap": "No test shown"}
    ],
    "overconfident": false,
    "overconfidence_detail": null,
    "pass2_model": "claude-opus-4-6",

    // From Pass 3 (Opus)
    "intent_reasoning": "User explicitly asked to fix the bug, not add validation",

    // From Pass 4 (Haiku)
    "importance_score": 8,
    "importance_reason": "Key architectural decision about pipeline structure"
  }
}</pre>
<p>Messages that weren't analyzed by any pass (tier 1 messages, user messages for passes 1-2) keep <code>llm_behavior: null</code>.</p>
</div>


<!-- ═══════════════════════════════════════════════════ -->
<h2 id="output-files">Output Files from prepare_agent_data.py</h2>
<p>After the enriched session is created, <code>prepare_agent_data.py</code> splits it into 7 focused files. Each file serves a different downstream consumer.</p>

<div class="card">
<table>
<tr><th>File</th><th>What's In It</th><th>Who Reads It</th></tr>
<tr><td class="field-name">session_summary.json</td><td>Everything except the messages array. Session stats, tier distribution, frustration peaks, top files, token usage.</td><td>All agents &mdash; gives them context about the session before they see any messages.</td></tr>
<tr><td class="field-name">tier2plus_messages.json</td><td>Full content of all tier 2+ messages.</td><td>Thread Analyst, Geological Reader &mdash; agents that need to read actual message content.</td></tr>
<tr><td class="field-name">tier4_priority_messages.json</td><td>Full content of tier 4 (priority) messages only.</td><td>Agents doing targeted deep analysis on the most important messages.</td></tr>
<tr><td class="field-name">conversation_condensed.json</td><td>ALL messages with shortened field names (<code>i</code>, <code>r</code>, <code>c</code>, <code>cl</code>, <code>t</code>) to save space. Includes full content.</td><td>Agents that need the complete conversation flow.</td></tr>
<tr><td class="field-name">user_messages_tier2plus.json</td><td>User messages only, tier 2+. Full content.</td><td>Frustration/reaction analysis agents.</td></tr>
<tr><td class="field-name">emergency_contexts.json</td><td>5-message windows around each emergency intervention. Full content of surrounding messages.</td><td>Agents analyzing what Claude did wrong before the user exploded.</td></tr>
<tr><td class="field-name">batches/batch_NNN.json</td><td>Tier 2+ messages split into batches of 30. For agents that process per-message.</td><td>Primitives Tagger, per-message LLM analysis.</td></tr>
</table>

<p style="margin-top: 1rem;"><strong>Safe files:</strong> Two additional files are created with profanity sanitized (all swear words replaced with <code>[expletive]</code>) and char-per-line encoding collapsed:</p>
<ul style="margin-left: 1.5rem;">
<li><code>safe_condensed.json</code> &mdash; Sanitized version of conversation_condensed. This is what most LLM agents read.</li>
<li><code>safe_tier4.json</code> &mdash; Sanitized version of tier4_priority_messages.</li>
</ul>
<p>The sanitization exists because raw chat history contains profanity that triggers Anthropic's content policy when fed to LLM agents. The safe files let agents analyze the session without getting blocked.</p>
</div>


<!-- ═══════════════════════════════════════════════════ -->
<h2 id="real-example">A Real Message: All Fields Populated</h2>
<p>This is an actual enriched message from session_0012ebed (message index 30), showing every field with real data. This message is from Claude Opus 4.5, summarizing 8 documentation modules it found.</p>

<div class="card">
<pre class="example-json">{
  "index": 30,
  "role": "assistant",
  "content": "Now I have a complete picture! This is fascinating - we built a comprehensive
              new documentation paradigm... [1,926 chars total]",
  "content_length": 1926,
  "content_length_raw": 1926,
  "content_hash": "42571b88504eae65",
  "timestamp": "2026-01-15T20:00:39.841000+00:00",
  "uuid": "bb1be39b-d9da-4382-96ff-af08660354c4",
  "model": "claude-opus-4-5-20251101",
  "has_thinking": false,
  "thinking_length": 0,

  "metadata": {
    "index": 30,
    "timestamp": "2026-01-15T20:00:39.841000+00:00",
    "hour": 20,                          // 8 PM
    "day": "Thursday",
    "length": 1926,
    "images": 0,
    "code_block": false,
    "code_langs": [],
    "files": [                           // 8 files mentioned
      "p05_drift_detector.py",
      "p05_redundancy_detector.py",
      "p05_boilerplate_detector.py",
      "p05_doc_code_verifier.py",
      "p05_code_journal_enforcer.py",
      "p05_doc_requirements.py",
      "p05_documentation_enforcer.py",
      "p05_doc_quality_analyzer.py"
    ],
    "paths": [],
    "error": false,                      // No actual error
    "error_types": [],
    "traceback": false,
    "browser_open": false,
    "server_cmd": false,
    "ports": [],
    "terminal_paste": false,
    "terminal_sessions": [],
    "python_scripts": [],
    "terminal_mentions": 0,
    "server_issue": false,
    "files_create": [],
    "files_open": [],
    "files_edit": [],
    "tools": [],
    "caps_ratio": 0.07,                  // Normal (not yelling)
    "exclamations": 2,
    "questions": 0,
    "profanity": false,
    "repeated_phrase": false,
    "repeat_count": 0,
    "emergency_intervention": false,
    "emergency_reason": ""
  },

  "filter_tier": 4,                      // PRIORITY (1926 chars > 500 threshold)
  "filter_tier_name": "PRIORITY",
  "filter_score": 37,                    // Very high
  "filter_signals": [
    "frustration:1",                     // 1 frustration keyword
    "failure:1",                         // 1 failure keyword
    "pivot:1",                           // 1 pivot keyword
    "architecture:5",                    // 5 architecture keywords!
    "code:12",                           // 12 code keywords
    "plan:2"                             // 2 planning keywords
  ],
  "filter_signals_content_referential": false,  // Not just discussing errors

  "behavior_flags": {
    "confusion": true,                   // "I'm not sure" equivalent detected
    "forgetting": false,
    "assumptions": false,
    "contradicts": false,
    "apologizes": false,
    "rushing": false,
    "overconfident": false,
    "ignores_context": false,
    "repeats_failed": false,
    "clarifying": false,
    "asks_questions": false,
    "acknowledges_mistake": false,
    "damage_score": 2                    // Low (confusion is -5, nets low)
  },

  "is_protocol": false,
  "protocol_type": null,
  "was_char_encoded": false,

  "llm_behavior": {                      // From Phase 0.5
    "content_referential": false,
    "content_referential_confidence": 0.9,
    "assumption_subtypes": {
      "code": false,
      "format": false,
      "direction": false,
      "scope": false
    },
    "pass1_model": "claude-haiku-4-5-20251001",
    "silent_decisions": [{
      "decision": "Framing '10% code, 90% comments' as user's vision",
      "context": "This was a quote from code, not the user's stated vision"
    }],
    "unverified_claims": [{
      "claim": "All 7 modules marked as Complete",
      "evidence_gap": "No test execution or verification shown"
    }],
    "overconfident": false,
    "pass2_model": "claude-opus-4-6",
    "importance_score": 8,               // High strategic importance
    "importance_reason": "Major insight summarizing 7 undiscovered modules"
  }
}</pre>
</div>


<!-- ═══════════════════════════════════════════════════ -->
<div class="issue-box">
<h3>Open Design Decision: Where Do LLM Results Live?</h3>
<p>Right now, when Phase 0.5 finishes all 4 LLM passes and merges the results, the output goes into a <strong>new file</strong> called <code>enriched_session_v2.json</code>. The original <code>enriched_session.json</code> is untouched.</p>

<p>This means the downstream safe files (<code>safe_condensed.json</code>, <code>safe_tier4.json</code>) still have <code>llm_behavior: null</code> on every message. Phase 1 agents that read the safe files don't see the LLM behavior data.</p>

<p><strong>Three options:</strong></p>

<div class="grid-2" style="margin-top: 1rem;">
<div class="card-inner">
<h4>Option A: Keep v2 separate (current)</h4>
<p>Safest. Original data never touched. But downstream agents can't use LLM behavior data unless they specifically read the v2 file.</p>
</div>
<div class="card-inner">
<h4>Option B: Overwrite original, regenerate safe files</h4>
<p>Merge writes back to <code>enriched_session.json</code>, then re-runs <code>prepare_agent_data.py</code> so safe files get populated <code>llm_behavior</code>. Downstream agents see everything. But the original pre-LLM data is gone.</p>
</div>
</div>
<div class="card-inner" style="margin-top: 0.5rem;">
<h4>Option C: Copy v2 to original, keep v2 as backup, regenerate safe files</h4>
<p>Best of both worlds. The original gets the LLM data, safe files get regenerated, and the v2 file stays as a backup. More disk usage but no data loss.</p>
</div>

<p style="margin-top: 1rem;"><strong>Estimated cost to run all 4 LLM passes across 162 sessions:</strong> ~$169 (not $77 as originally estimated). The difference is because Opus passes on larger sessions cost more than anticipated. 12,766 tier-2+ messages need analysis, and the biggest sessions have over 2,000 messages each.</p>
</div>


<!-- ═══════════════════════════════════════════════════ -->
<h2 id="session-stats">Session-Level Statistics</h2>
<p>In addition to per-message enrichment, Phase 0 accumulates session-wide statistics in the <code>session_stats</code> object:</p>

<div class="card">
<table>
<tr><th>Field</th><th>What It Is</th></tr>
<tr><td class="field-name">session_id</td><td>The session UUID (or short ID).</td></tr>
<tr><td class="field-name">total_messages</td><td>Total messages in the session (all roles).</td></tr>
<tr><td class="field-name">user_messages</td><td>Count of user-role messages.</td></tr>
<tr><td class="field-name">assistant_messages</td><td>Count of assistant-role messages.</td></tr>
<tr><td class="field-name">human_messages</td><td>Actual human-typed messages (user messages minus protocol messages). This is the real conversation length.</td></tr>
<tr><td class="field-name">protocol_messages</td><td>Count of system-generated protocol messages detected and suppressed.</td></tr>
<tr><td class="field-name">tier_distribution</td><td>Object with counts per tier: <code>{"1_skip": 1111, "2_basic": 23, "3_standard": 80, "4_priority": 103}</code>.</td></tr>
<tr><td class="field-name">frustration_peaks</td><td>Array of moments where user frustration spiked: messages with caps ratio above 0.3 or profanity. Each entry has the message index, caps ratio, profanity flag, and a content preview.</td></tr>
<tr><td class="field-name">file_mention_counts</td><td>Dictionary of filename &rarr; mention count across the whole session. After false positive removal.</td></tr>
<tr><td class="field-name">top_files</td><td>The 30 most-mentioned files, sorted by count.</td></tr>
<tr><td class="field-name">error_count</td><td>Total messages with real errors detected (after protocol suppression and content-referential correction).</td></tr>
<tr><td class="field-name">tool_failure_count</td><td>Messages from synthetic model (<code>model="&lt;synthetic&gt;"</code>) that contain error keywords &mdash; these are tool calls that failed.</td></tr>
<tr><td class="field-name">emergency_interventions</td><td>Array of emergency moments: the message index, the reason code, and a content preview.</td></tr>
<tr><td class="field-name">is_subagent</td><td>Whether this session is a subagent (memory agent, observer, task agent) rather than a human conversation.</td></tr>
<tr><td class="field-name">agent_id</td><td>The subagent's ID, if applicable.</td></tr>
<tr><td class="field-name">agent_type</td><td>Type: "memory_agent", "observer_agent", "task_agent", or null.</td></tr>
<tr><td class="field-name">char_per_line_messages</td><td>How many messages had char-per-line encoding and were collapsed.</td></tr>
<tr><td class="field-name">total_input_tokens</td><td>Total API input tokens used during the original session.</td></tr>
<tr><td class="field-name">total_output_tokens</td><td>Total API output tokens.</td></tr>
<tr><td class="field-name">total_thinking_chars</td><td>Total characters of thinking content across all messages.</td></tr>
<tr><td class="field-name">false_positive_files_removed</td><td>Files that were detected by regex but removed as false positives (generic names like file.py, test.py, or short names that are substrings of longer detected names).</td></tr>
</table>
</div>


<!-- ═══════════════════════════════════════════════════ -->
<h2>Source Files</h2>
<div class="card">
<table>
<tr><th>File</th><th>Lines</th><th>Purpose</th></tr>
<tr><td><code>phase_0_prep/deterministic_prep.py</code></td><td>619</td><td>Main Phase 0 script. Orchestrates the full per-message enrichment pipeline.</td></tr>
<tr><td><code>phase_0_prep/metadata_extractor.py</code></td><td>909</td><td>35 regex patterns &rarr; 34 metadata fields per message.</td></tr>
<tr><td><code>phase_0_prep/message_filter.py</code></td><td>332</td><td>7 keyword categories &rarr; 4-tier classification.</td></tr>
<tr><td><code>phase_0_prep/claude_behavior_analyzer.py</code></td><td>850</td><td>20 behavioral patterns &rarr; behavior_flags + user upset score.</td></tr>
<tr><td><code>phase_0_prep/prepare_agent_data.py</code></td><td>246</td><td>Splits enriched output into 7+ focused files with profanity sanitization.</td></tr>
<tr><td><code>phase_0_prep/prompts.py</code></td><td>381</td><td>4 LLM pass prompt templates + PASS_CONFIGS registry.</td></tr>
<tr><td><code>phase_0_prep/llm_pass_runner.py</code></td><td>551</td><td>Batching, context window management, API calls, retry logic, JSON recovery.</td></tr>
<tr><td><code>phase_0_prep/merge_llm_results.py</code></td><td>231</td><td>Merges 4 pass outputs into enriched_session_v2.json.</td></tr>
<tr><td><code>phase_0_prep/batch_llm_orchestrator.py</code></td><td>321</td><td>Runs all passes across 162 sessions with concurrent processing.</td></tr>
</table>
</div>

<p style="margin-top: 3rem; color: var(--text-muted); text-align: center; font-size: 0.85rem;">
  Generated 2026-02-19. Source: phase_0_prep/*.py from the Hyperdocs pipeline.
</p>

</body>
</html>
