<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Hyperdocs Pipeline — Complete System Documentation</title>
<style>
* { margin:0; padding:0; box-sizing:border-box; }
body { font-family:'SF Mono','Fira Code','JetBrains Mono',monospace; background:#0d1117; color:#c9d1d9; padding:32px; line-height:1.7; }
h1 { color:#58a6ff; font-size:24px; margin-bottom:4px; }
.sub { color:#8b949e; font-size:12px; margin-bottom:32px; }

.grid { display:grid; grid-template-columns:repeat(auto-fit, minmax(140px, 1fr)); gap:12px; margin:16px 0 32px; }
.card { background:#161b22; border:1px solid #30363d; border-radius:8px; padding:14px; text-align:center; }
.card .n { font-size:28px; font-weight:700; }
.card .l { font-size:10px; color:#8b949e; text-transform:uppercase; letter-spacing:1px; margin-top:4px; }

.phase { margin:32px 0; }
.phase-header {
  display:flex; align-items:center; gap:12px; padding:12px 16px;
  border-radius:8px 8px 0 0; color:#0d1117;
}
.phase-num { font-size:16px; font-weight:700; }
.phase-name { font-size:18px; font-weight:600; }
.phase-cost { margin-left:auto; font-size:11px; opacity:0.8; }

.step {
  background:#161b22; border:1px solid #30363d; border-top:none;
  padding:20px; margin:0;
}
.step:last-child { border-radius:0 0 8px 8px; }
.step-header {
  display:flex; align-items:center; gap:10px; margin-bottom:12px;
  padding-bottom:8px; border-bottom:1px solid #21262d;
}
.step-num {
  background:#21262d; color:#58a6ff; padding:2px 10px;
  border-radius:10px; font-size:11px; font-weight:700;
}
.step-name { color:#f0f6fc; font-weight:600; font-size:14px; }
.step-file { margin-left:auto; color:#484f58; font-size:10px; }

.io { margin:12px 0; padding:10px 14px; background:#0d1117; border-radius:6px; font-size:11px; }
.input { color:#3fb950; margin-bottom:4px; }
.output { color:#f85149; }

ol.subpoints { padding-left:24px; margin:12px 0; }
ol.subpoints li {
  margin:8px 0; font-size:12px; color:#c9d1d9; line-height:1.7;
}
ol.subpoints li ul {
  margin:6px 0 6px 16px; list-style-type:disc;
}
ol.subpoints li ul li {
  margin:4px 0; font-size:11px; color:#8b949e;
}
ol.subpoints li strong { color:#f0f6fc; }
</style>
</head>
<body>

<h1>Hyperdocs Pipeline — Complete System Documentation</h1>
<div class="sub">Revised 2026-02-15 00:26 | 42 Python files | 16,390 lines | 162 unique sessions | 119 duplicates excluded</div>

<div class="grid">
  <div class="card"><div class="n" style="color:#58a6ff">42</div><div class="l">Python Files</div></div>
  <div class="card"><div class="n" style="color:#f0f6fc">16,390</div><div class="l">Lines of Code</div></div>
  <div class="card"><div class="n" style="color:#3fb950">162</div><div class="l">Unique Sessions</div></div>
  <div class="card"><div class="n" style="color:#d29922">166</div><div class="l">API Prompts</div></div>
  <div class="card"><div class="n" style="color:#f85149">664</div><div class="l">Total API Calls</div></div>
  <div class="card"><div class="n" style="color:#bc8cff">2,556</div><div class="l">Commitment Tokens</div></div>
</div>


<div class="phase">
<div class="phase-header" style="background:#3fb950">
  <span class="phase-num">Phase 0</span>
  <span class="phase-name">Deterministic Prep</span>
  <span class="phase-cost">$0 (pure Python, no API calls)</span>
</div>

<div class="step">
<div class="step-header">
  <span class="step-num">Step 1</span>
  <span class="step-name">Parse and Enrich</span>
  <span class="step-file">phase_0_prep/deterministic_prep.py</span>
</div>
<div class="io">
  <div class="input">INPUT: Raw JSONL chat history file from ~/.claude/projects/ or ~/PERMANENT_CHAT_HISTORY/sessions/</div>
  <div class="output">OUTPUT: enriched_session.json — every message with full metadata, tiers, behavior flags, protocol detection</div>
</div>
<ol class="subpoints">
  <li>Read JSONL file line by line using ClaudeSessionReader. Each line is a JSON object representing one message in the conversation.</li>
  <li>For each message, extract: role (user or assistant), content (the actual text), timestamp, thinking blocks (Claude's internal reasoning), tool calls, and model identifier.</li>
  <li>Count output tokens using tiktoken (the cl100k_base encoder). This replaces the unreliable output_tokens field in the JSONL, which is a streaming snapshot that often shows 1-3 tokens for messages that actually contain thousands.</li>
  <li>Detect protocol messages — system-generated content that is not part of the actual human-AI conversation. Five types are detected:
    <ul>
      <li><strong>Empty wrappers</strong> — zero-length messages that are streaming delimiters or tool result containers</li>
      <li><strong>XML protocol tags</strong> — system-reminder, command-name, command-message, local-command-stdout, local-command-stderr, task-notification tags</li>
      <li><strong>/clear continuation boilerplate</strong> — summaries injected when a session runs out of context and gets reset</li>
      <li><strong>Skill injections</strong> — content from .claude/skills/ injected when the user invokes a slash command like /pdf or /xlsx</li>
      <li><strong>Subagent relay messages</strong> — automated observation relays from memory agent or observer agent sessions ("Hello memory agent", "PROGRESS SUMMARY CHECKPOINT")</li>
    </ul>
  </li>
  <li>Collapse character-per-line encoding. Some messages have every character separated by a newline (A\nL\nW\nA\nY\nS instead of ALWAYS). This is detected by checking if more than 70% of lines are single characters, then the newlines are removed to recover the original text.</li>
  <li>After collapsing, re-check for protocol markers on the collapsed content. Some continuation summaries are char-encoded and only detectable after decoding.</li>
  <li>Extract metadata using MetadataExtractor — 50+ signals per message including: files mentioned, paths mentioned, error types (TypeError, ValueError, etc.), traceback presence, code blocks and languages, browser activity, server commands, port numbers, terminal paste detection, terminal session IDs, Python scripts run, file operation requests (create, open, edit), tool calls, caps ratio, exclamation count, question count, profanity detection, repeated phrase detection, and emergency intervention detection.</li>
  <li>Classify each message into a processing tier using MessageFilter:
    <ul>
      <li><strong>Tier 1 (SKIP)</strong> — under 50 characters with no importance signals</li>
      <li><strong>Tier 2 (BASIC)</strong> — 50-100 characters or has some keyword signals</li>
      <li><strong>Tier 3 (STANDARD)</strong> — 100-500 characters or has pasted content or moderate signal density</li>
      <li><strong>Tier 4 (PRIORITY)</strong> — 500+ characters or high signal density or multiple struggle signals</li>
    </ul>
  </li>
  <li>Detect content-referential signals. When an assistant message discusses failure handling or error patterns, the keyword-based signals (frustration:N, failure:N) describe the content topic, not the session dynamics. Three detection strategies:
    <ul>
      <li>Analytical indicators on assistant messages (2+ indicators like "problem", "issue", "failure mode" in content over 500 chars)</li>
      <li>Signal density anomaly (failure count over 20 or frustration count over 10 on messages over 1000 chars)</li>
      <li>Positive-tone assistant content (failure/frustration signals combined with implementation language like "created", "built", "step 1" in content over 300 chars)</li>
    </ul>
  </li>
  <li>Analyze Claude's behavior using ClaudeBehaviorAnalyzer — 20 behavioral patterns in 4 categories, each scored by user upset level:
    <ul>
      <li><strong>Context damage:</strong> confusion (-5, good), forgetting (0), assumptions (7), contradicts self (0), apologizes (0), rushing (4), overconfident (8)</li>
      <li><strong>Idea Evolution patterns:</strong> unsolicited addition (7), premature completion (9), batch without verify (10), silent decision (10), hollow fulfillment (10), unverified claim/lying (10), scope creep (10), repeats failed approach (9)</li>
      <li><strong>Recovery behaviors (positive):</strong> clarifying, asks questions, acknowledges mistake</li>
      <li><strong>Context awareness:</strong> ignores context (20 — the single worst behavior)</li>
    </ul>
    The analyzer uses all previous messages in the session for cross-message detection (contradictions, repeated failures), falling back to 10 messages if the full history causes performance issues.
  </li>
  <li>Detect subagent sessions using three strategies: session ID pattern (_agent- or agent- prefix), JSONL filename pattern (parent ID prepended to child UUID), and content-based detection (first 5 messages checked for "Hello memory agent" or "PROGRESS SUMMARY CHECKPOINT" after char-per-line decoding).</li>
  <li>Suppress all extracted signals on protocol messages. Continuation summaries contain quoted content from prior sessions — the metadata extractor finds errors, profanity, and frustration in that quoted text, but none of it is from the current session. All signals are zeroed out on protocol messages.</li>
  <li>Force protocol messages to tier 1 regardless of content richness. A continuation summary may have high signal density, but it's system-generated context, not human input.</li>
  <li>Detect and remove false positive file mentions using two strategies: a blocklist of generic filenames (file.py, mentioned.py, etc.) and substring matching where short names that are substrings of longer detected names are removed.</li>
  <li>Separate mentioned vs encountered errors. If an assistant message discusses error handling (content-referential) or is long analytical content, error flags are downgraded to "mentioned_not_encountered" rather than treated as actual errors in the session.</li>
  <li>Accumulate session-level statistics: tier distribution, frustration peaks (user messages only, not protocol, with profanity or high caps ratio), emergency interventions (excluding protocol), file mention counts, error counts, tool failure counts, token usage totals, subagent detection results, and char-per-line message counts.</li>
  <li>Build the enriched record for each message containing: index, role, full content, content length (corrected for encoding), raw content length, content hash (SHA256 first 16 hex chars), timestamp, UUID, model, thinking presence and length, metadata dict, filter tier and name, filter score, filter signals, content-referential flag, behavior flags with user upset score, protocol detection, and char-encoding flag.</li>
  <li>Write enriched_session.json — the complete session with all metadata, one file per session.</li>
</ol>
</div>

<div class="step">
<div class="step-header">
  <span class="step-num">Step 2</span>
  <span class="step-name">Clean and Package for Agents</span>
  <span class="step-file">phase_0_prep/prepare_agent_data.py</span>
</div>
<div class="io">
  <div class="input">INPUT: enriched_session.json from Step 1</div>
  <div class="output">OUTPUT: session_summary.json, tier2plus_messages.json, tier4_priority_messages.json, conversation_condensed.json, user_messages_tier2plus.json, emergency_contexts.json, batches/*.json, safe_tier4.json, safe_condensed.json</div>
</div>
<ol class="subpoints">
  <li>Extract session summary — all session-level statistics without any message content. This gives agents a high-level overview without consuming tokens on individual messages.</li>
  <li>Filter tier 2+ messages into a separate file for deep analysis. These are the messages with enough content and signal density to be worth analyzing.</li>
  <li>Filter tier 4 (priority) messages into their own file — the most important messages in the session.</li>
  <li>Build conversation_condensed.json with cleaned content for all messages. Cleaning applies four rules decided by the user on Feb 13, 2026:
    <ul>
      <li><strong>Rule 1:</strong> Decode character-per-line encoding (H\nE\nL\nL\nO becomes HELLO) — recovers 50% of characters on affected messages</li>
      <li><strong>Rule 2:</strong> Collapse triple-or-more consecutive newlines to double newlines — removes blank line waste</li>
      <li><strong>Rule 3:</strong> Replace decorative Unicode box-drawing lines (rows of ─━═ characters) with simple --- separators</li>
      <li><strong>Rule 4:</strong> No content is truncated. Full message text is preserved after cleaning.</li>
    </ul>
  </li>
  <li>Filter user-only tier 2+ messages for frustration and reaction analysis.</li>
  <li>Build emergency context windows — for each emergency intervention, capture the 5 messages before and 5 messages after for context.</li>
  <li>Split tier 2+ messages into batches of 30 for per-message processing by agents that need to analyze messages individually.</li>
  <li>Sanitize profanity in two passes:
    <ul>
      <li><strong>Pass 1:</strong> Sanitize all JSON files already written to disk (replacing profanity with [expletive] for API content policy compliance)</li>
      <li><strong>Pass 2:</strong> Sanitize in-memory data (tier4 and condensed lists) so the safe files generated next also have clean content</li>
    </ul>
  </li>
  <li>Build safe_tier4.json and safe_condensed.json — the files that agents actually read. These apply all 7 user-approved data cleaning decisions:
    <ul>
      <li><strong>Decision 1 (char-decoding):</strong> All character-per-line encoding decoded</li>
      <li><strong>Decision 2 (empty wrappers):</strong> Excluded from agent data entirely — zero content, zero value</li>
      <li><strong>Decision 3 (continuation summaries):</strong> Kept and tagged as message_category="session_context" — agents can distinguish them from actual conversation turns</li>
      <li><strong>Decision 4 (task notifications):</strong> Kept in full as message_category="task_milestone" — execution timeline data</li>
      <li><strong>Decision 5 (formatting):</strong> Triple newlines collapsed, decorative Unicode replaced</li>
      <li><strong>Decision 6 (profanity):</strong> Sanitized for API compatibility</li>
      <li><strong>Decision 7 (duplicates):</strong> Identical messages deduplicated — first occurrence kept, later copies removed</li>
    </ul>
  </li>
  <li>Report what was excluded: count of empty wrappers removed and duplicates removed.</li>
</ol>
</div>
</div>

<div class="phase">
<div class="phase-header" style="background:#58a6ff">
  <span class="phase-num">Phase 1</span>
  <span class="phase-name">Opus Agent Extraction</span>
  <span class="phase-cost">4 Opus API calls per session (adaptive thinking, 1M context)</span>
</div>

<div class="step">
<div class="step-header">
  <span class="step-num">Pre-processing</span>
  <span class="step-name">Session Measurement and Chunking</span>
  <span class="step-file">phase1_redo_orchestrator.py</span>
</div>
<div class="io">
  <div class="input">INPUT: safe_condensed.json from Phase 0, session_measurements.json from pre-measurement scan</div>
  <div class="output">OUTPUT: Deterministic chunk plan per session — which messages go in which prompt</div>
</div>
<ol class="subpoints">
  <li>Load the full 12 commitments from ~/.claude/CLAUDE.md (2,556 tokens). These are prepended to every single API call — non-negotiable.</li>
  <li>Calculate the per-prompt token budget: 872,000 tokens (API input limit) minus 2,556 (commitments) minus the agent instruction tokens (222-758 depending on agent) = approximately 868,686 tokens available for message content.</li>
  <li>Pre-measure every message in the session using tiktoken. Each message is serialized to JSON and its exact token count is recorded. No estimation — pure math.</li>
  <li>Identify duplicate sessions using JSONL filename analysis. The chat history archive contains 625 UUIDs that appear in two JSONL files (one with a parent prefix, one without). 119 of the 285 session directories are duplicates. These are skipped entirely.</li>
  <li>For each unique session, determine if it fits in a single prompt or needs chunking:
    <ul>
      <li><strong>160 sessions</strong> fit in a single prompt — no chunking needed</li>
      <li><strong>2 sessions</strong> need multiple prompts — one needs 4 chunks, one needs 2 chunks</li>
    </ul>
  </li>
  <li>Chunking rules (decided by user):
    <ul>
      <li>Fill each prompt with whole messages only. A message is never split across prompts.</li>
      <li>If a message doesn't fit in the remaining space, it becomes the first message of the next prompt.</li>
      <li>One session per prompt. Sessions are never mixed.</li>
      <li>Every prompt includes the full commitments text and the agent's instruction text.</li>
    </ul>
  </li>
  <li>Total work: 162 unique sessions, 166 prompts, 664 API calls (4 agents x 166 prompts).</li>
</ol>
</div>

<div class="step">
<div class="step-header">
  <span class="step-num">Step 1</span>
  <span class="step-name">Thread Analyst</span>
  <span class="step-file">phase1_redo_orchestrator.py (thread_analyst_prompt)</span>
</div>
<div class="io">
  <div class="input">INPUT: Commitments + safe_condensed.json + safe_tier4.json + session_summary.json (full cleaned content, no truncation)</div>
  <div class="output">OUTPUT: thread_extractions.json — 6 analytical threads with message-level entries</div>
</div>
<ol class="subpoints">
  <li>The full 12 commitments are prepended to the prompt before any instruction or data.</li>
  <li>Opus extracts 6 analytical threads from the session: ideas (what the user is building/thinking), reactions (how the user responded to Claude's actions), software (files created/modified/deleted), code (specific code blocks and functions affected), plans (detected plans with completed/pending items), and behavior (Claude's behavioral patterns including harmful ones).</li>
  <li>Each thread contains entries with: message index (must reference actual indices from the data, never fabricated), description, and significance rating.</li>
  <li>For chunked sessions: the Thread Analyst runs once per chunk, and results are merged by combining all entries across chunks for each thread name.</li>
  <li>JSON response is parsed with 3-strategy recovery: outermost braces extraction, trailing comma removal, and brace completion for truncated output.</li>
  <li>On API overload or connection error: retry up to 3 times with 30/60/90 second backoff.</li>
  <li>Uses adaptive thinking (model decides its own thinking allocation) and the 1M context beta header (context-1m-2025-08-07).</li>
</ol>
</div>

<div class="step">
<div class="step-header">
  <span class="step-num">Step 2</span>
  <span class="step-name">Geological Reader</span>
  <span class="step-file">phase1_redo_orchestrator.py (geological_reader_prompt)</span>
</div>
<div class="io">
  <div class="input">INPUT: Commitments + safe_condensed.json + safe_tier4.json + session_summary.json</div>
  <div class="output">OUTPUT: geological_notes.json — multi-resolution temporal analysis</div>
</div>
<ol class="subpoints">
  <li>The full 12 commitments are prepended to the prompt.</li>
  <li>Opus performs multi-resolution temporal analysis at three zoom levels:
    <ul>
      <li><strong>Micro</strong> — individual message observations (single message significance)</li>
      <li><strong>Meso</strong> — 5-message window observations (patterns across short sequences)</li>
      <li><strong>Macro</strong> — 15-20 message arc observations (session-wide structures and phase boundaries)</li>
    </ul>
  </li>
  <li>Identifies temporal gaps (significant pauses in the conversation), phase boundaries (where the work shifts from one topic to another), and strata transitions (where the geological metaphor of the session changes character).</li>
  <li>Generates a geological metaphor summarizing the session's structure — describing the session as a geological formation with layers, intrusions, and formations.</li>
  <li>For chunked sessions: runs per chunk, results merged by combining micro/meso/macro/observations lists across chunks.</li>
  <li>Same retry logic and adaptive thinking as all other agents.</li>
</ol>
</div>

<div class="step">
<div class="step-header">
  <span class="step-num">Step 3</span>
  <span class="step-name">Primitives Tagger</span>
  <span class="step-file">phase1_redo_orchestrator.py (primitives_tagger_prompt)</span>
</div>
<div class="io">
  <div class="input">INPUT: Commitments + tier 2+ messages with full content from safe_tier4.json + session_summary.json</div>
  <div class="output">OUTPUT: semantic_primitives.json — 7 semantic primitives per tier 2+ message</div>
</div>
<ol class="subpoints">
  <li>The full 12 commitments are prepended to the prompt.</li>
  <li>Only tier 2+ messages are sent (tier 1 messages are too short for meaningful classification). Full cleaned content is included for each message — no truncation.</li>
  <li>The prompt includes three strict rules for the tagger:
    <ul>
      <li><strong>RULE 1 — DIFFERENTIATE:</strong> Do not assign the same tags to every message. If 3+ consecutive messages get identical primitives, stop and re-read. Monotonous tagging is the most common failure mode.</li>
      <li><strong>RULE 2 — IGNORE filter_signals:</strong> Keyword counts describe the content topic, not the author's state. Check the content_ref field — if true, treat signals as analytical, not emotional.</li>
      <li><strong>RULE 3 — Role matters:</strong> User messages and assistant messages should never get the same tags. A user saying "fix this" is decided/frustrated/bugfix. An assistant saying "I've fixed it" is modified/working/confident/bugfix.</li>
    </ul>
  </li>
  <li>Opus tags each message with 7 semantic primitives:
    <ul>
      <li><strong>action_vector:</strong> created, modified, debugged, refactored, discovered, decided, abandoned, reverted</li>
      <li><strong>confidence_signal:</strong> experimental, tentative, working, stable, proven, fragile</li>
      <li><strong>emotional_tenor:</strong> frustrated, uncertain, curious, cautious, confident, excited, relieved</li>
      <li><strong>intent_marker:</strong> correctness, performance, maintainability, feature, bugfix, exploration, cleanup</li>
      <li><strong>friction_log:</strong> single sentence describing what went wrong or caused friction</li>
      <li><strong>decision_trace:</strong> "chose X over Y because Z" format</li>
      <li><strong>disclosure_pointer:</strong> session_id:msgN reference</li>
    </ul>
  </li>
  <li>For single-prompt sessions: if Opus stops before tagging all messages, a continuation loop sends the remaining message indices in follow-up calls (up to 10 rounds). Results from all rounds are merged and sorted by message index.</li>
  <li>For chunked sessions: each chunk is tagged separately, then all tagged messages are merged and sorted.</li>
</ol>
</div>

<div class="step">
<div class="step-header">
  <span class="step-num">Step 4</span>
  <span class="step-name">Explorer + Verification</span>
  <span class="step-file">phase1_redo_orchestrator.py (explorer_verification_prompt)</span>
</div>
<div class="io">
  <div class="input">INPUT: Commitments + safe_condensed.json + safe_tier4.json + session_summary.json + thread_extractions.json + geological_notes.json + semantic_primitives.json</div>
  <div class="output">OUTPUT: explorer_notes.json — free-form observations + quality verification of all prior outputs</div>
</div>
<ol class="subpoints">
  <li>Runs LAST deliberately. It reads all three prior agents' outputs plus the Phase 0 data. This is the quality control step.</li>
  <li>The full 12 commitments are prepended to the prompt.</li>
  <li>Opus makes free-form observations about the session — things the other three agents might have missed, cross-references, meta-observations.</li>
  <li>Verifies Phase 0 data quality: checks protocol detection accuracy, content-referential flag correctness, tier classification appropriateness, frustration peak validity.</li>
  <li>Verifies Thread Analyst output: checks for coverage gaps (ranges of messages with no thread entries), fabricated message indices, content-referential signal misreads.</li>
  <li>Verifies Geological Reader output: checks observation accuracy (do the claimed patterns match the data?), range coverage (are there gaps?), metaphor relevance.</li>
  <li>Verifies Primitives Tagger output: checks for tag monotony (too many identical tags), content-referential misinterpretation, coverage completeness (were all tier 2+ messages tagged?), role differentiation (are user and assistant messages getting different tags?).</li>
  <li>Rates overall data quality as one of three levels: clean (no issues found), minor_issues (small imperfections), or significant_issues (problems that affect downstream analysis).</li>
  <li>For chunked sessions: the Explorer receives the merged results from all chunks, not per-chunk data. It runs as a single call regardless of chunking.</li>
</ol>
</div>
</div>

<div class="phase">
<div class="phase-header" style="background:#bc8cff">
  <span class="phase-num">Phase 2</span>
  <span class="phase-name">Synthesis</span>
  <span class="phase-cost">Opus API + Python</span>
</div>

<div class="step">
<div class="step-header">
  <span class="step-num">Step 1</span>
  <span class="step-name">Idea Graph Builder</span>
  <span class="step-file">Opus agent (launched by orchestrator)</span>
</div>
<div class="io">
  <div class="input">INPUT: thread_extractions.json, semantic_primitives.json, session_summary.json</div>
  <div class="output">OUTPUT: idea_graph.json — directed graph of idea evolution</div>
</div>
<ol class="subpoints">
  <li>Opus builds a directed graph where each node is an "idea-state" — a snapshot of an idea at a specific moment in the session, with a confidence level and emotional context.</li>
  <li>Each node has: name, description, confidence (experimental through proven), first appearance message, last appearance message, and related files.</li>
  <li>Each edge is a transition between idea-states, classified into one of 10 types: evolved, pivoted, split, merged, abandoned, resurrected, constrained, expanded, concretized, abstracted.</li>
  <li>Each edge has: from_id, to_id, transition_type, trigger_message (what caused the transition), and evidence (the text that supports this classification).</li>
  <li>The graph is not a timeline — it's a topology. Ideas can split, merge, and loop. The shape of the graph tells the story of how the user's thinking evolved.</li>
</ol>
</div>

<div class="step">
<div class="step-header">
  <span class="step-num">Step 2</span>
  <span class="step-name">6-Pass Synthesis</span>
  <span class="step-file">Opus agent (launched by orchestrator)</span>
</div>
<div class="io">
  <div class="input">INPUT: thread_extractions.json, geological_notes.json, semantic_primitives.json, idea_graph.json</div>
  <div class="output">OUTPUT: synthesis.json, grounded_markers.json</div>
</div>
<ol class="subpoints">
  <li>Pass 1 (temperature 0.3): Conservative structural analysis — identifies the factual skeleton of the session.</li>
  <li>Pass 2 (temperature 0.5): Pattern recognition — finds recurring themes and behavioral patterns.</li>
  <li>Pass 3 (temperature 0.7): Creative connections — draws links between ideas that aren't explicitly stated.</li>
  <li>Pass 4 (temperature 0.9): Speculative insights — proposes hypotheses about why things happened the way they did.</li>
  <li>Pass 5 (temperature 1.0): Free association — unconstrained exploration of the data.</li>
  <li>Pass 6 (temperature 0): Grounding pass — takes all metaphors and creative language from passes 1-5 and TRANSLATES them into practical developer guidance. This is translation, not summarization. Every metaphor becomes a concrete recommendation.</li>
</ol>
</div>

<div class="step">
<div class="step-header">
  <span class="step-num">Step 3</span>
  <span class="step-name">File Genealogy</span>
  <span class="step-file">phase_2_synthesis/file_genealogy.py</span>
</div>
<div class="io">
  <div class="input">INPUT: thread_extractions.json, idea_graph.json</div>
  <div class="output">OUTPUT: file_genealogy.json — file families and standalone files</div>
</div>
<ol class="subpoints">
  <li>Build activity timelines from thread extraction software entries — which files were created, modified, and deleted at which message indices.</li>
  <li>Detect idea graph lineage: if concept A mentions file X and concept A evolved into concept B which mentions file Y, then X and Y are lineage-linked.</li>
  <li>Detect temporal succession: if file X stops being modified and file Y starts within 5 messages, they may be the same concept under a new name.</li>
  <li>Detect name similarity: files with overlapping filename stems (thread_extractor and six_thread_extractor share "thread_extractor").</li>
  <li>Cluster files into families using union-find with strict merge rules: idea graph links alone are sufficient to merge, temporal alone is not, temporal + name is sufficient, name containment (one filename is a substring of another) is sufficient.</li>
  <li>Each family gets a concept name derived from the latest file, with versions ordered by first appearance and the most recently modified file marked as "current".</li>
</ol>
</div>

<div class="step">
<div class="step-header">
  <span class="step-num">Step 4</span>
  <span class="step-name">Code Similarity</span>
  <span class="step-file">phase_2_synthesis/code_similarity.py</span>
</div>
<div class="io">
  <div class="input">INPUT: All Python files in the project</div>
  <div class="output">OUTPUT: code_similarity_index.json — pairwise similarity scores and pattern classifications</div>
</div>
<ol class="subpoints">
  <li>Extract a fingerprint per file using AST parsing: top-level function names, class names and method signatures, imported module names, constants (ALL_CAPS names), and string literals over 10 characters.</li>
  <li>Compare all file pairs on multiple dimensions: function name overlap ratio, import overlap ratio, text similarity (using difflib SequenceMatcher on full source code), and containment check (is one file's function set a subset of the other's).</li>
  <li>Classify each matching pair into one of 8 pattern types:
    <ul>
      <li><strong>dead_copy</strong> — over 90% text similarity (identical or near-identical files)</li>
      <li><strong>evolution_pair</strong> — 60-90% text similarity (one file evolved from the other)</li>
      <li><strong>function_clone</strong> — over 50% shared function names with under 90% text similarity</li>
      <li><strong>partial_extraction</strong> — over 80% of one file's functions are a subset of the other's</li>
      <li><strong>template_variant</strong> — similar structure but under 30% text similarity (parameterized templates)</li>
      <li><strong>import_twin</strong> — over 70% shared imports with under 30% function overlap</li>
      <li><strong>name_only</strong> — filename stems overlap but code doesn't (under 5% any signal)</li>
      <li><strong>interface_mismatch</strong> — files reference each other's names but signatures diverge</li>
    </ul>
  </li>
</ol>
</div>
</div>

<div class="phase">
<div class="phase-header" style="background:#d29922">
  <span class="phase-num">Phase 3</span>
  <span class="phase-name">Hyperdoc Writing</span>
  <span class="phase-cost">Opus API</span>
</div>

<div class="step">
<div class="step-header">
  <span class="step-num">Step 1</span>
  <span class="step-name">File Mapper</span>
  <span class="step-file">Opus agent (one per session)</span>
</div>
<div class="io">
  <div class="input">INPUT: All Phase 1 + Phase 2 outputs for a session</div>
  <div class="output">OUTPUT: file_dossiers.json, claude_md_analysis.json</div>
</div>
<ol class="subpoints">
  <li>One Opus agent per session reads all analysis outputs and maps them to specific code files.</li>
  <li>Produces per-file dossiers containing: edit timeline (when the file was created, modified, deleted across the session), behavioral profile (what Claude was doing when it touched this file), and CLAUDE.md impact analysis (how the session's findings relate to the project instructions).</li>
</ol>
</div>

<div class="step">
<div class="step-header">
  <span class="step-num">Step 2</span>
  <span class="step-name">Cross-Session Aggregation</span>
  <span class="step-file">phase_4_hyperdoc_writing/aggregate_dossiers.py</span>
</div>
<div class="io">
  <div class="input">INPUT: file_dossiers.json from ALL sessions</div>
  <div class="output">OUTPUT: cross_session_file_index.json, per-file hyperdoc input extracts</div>
</div>
<ol class="subpoints">
  <li>Normalize the two incompatible dossier schemas (dict format from 56% of sessions, list format from 44%) into a single unified structure.</li>
  <li>Build a cross-session file index: for every file mentioned across all sessions, record which sessions mention it and what each session's dossier says about it.</li>
  <li>Generate per-file input extracts for files that appear in 3 or more sessions — these are the files with enough cross-session history to warrant a rich hyperdoc.</li>
</ol>
</div>

<div class="step">
<div class="step-header">
  <span class="step-num">Step 3</span>
  <span class="step-name">Per-File Hyperdoc Writer</span>
  <span class="step-file">Opus agent (one per file)</span>
</div>
<div class="io">
  <div class="input">INPUT: Per-file input extract + cross-session data</div>
  <div class="output">OUTPUT: One hyperdoc JSON per file</div>
</div>
<ol class="subpoints">
  <li>One Opus agent per file (456 files processed in the initial batch).</li>
  <li>Each hyperdoc has three sections: header (file-level metadata and story arc), inline function annotations (per-function context from the idea evolution), and footer (cross-session summary and genealogy).</li>
  <li>Uses @ctx annotation format for machine-readable metadata that tools can parse.</li>
  <li>Includes story arcs (the narrative of how this file evolved across sessions), friction logs (what went wrong), decision traces (choices made and alternatives considered), and genealogy links (what file this evolved from or was replaced by).</li>
</ol>
</div>
</div>

<div class="phase">
<div class="phase-header" style="background:#f0f6fc">
  <span class="phase-num" style="color:#0d1117">Phase 4</span>
  <span class="phase-name" style="color:#0d1117">Insertion</span>
  <span class="phase-cost" style="color:#0d1117">$0 (pure Python)</span>
</div>

<div class="step">
<div class="step-header">
  <span class="step-num">Step 1</span>
  <span class="step-name">Layer Management</span>
  <span class="step-file">phase_4_insertion/hyperdoc_layers.py</span>
</div>
<div class="io">
  <div class="input">INPUT: Existing hyperdocs + new hyperdoc data from Phase 3</div>
  <div class="output">OUTPUT: Updated layered hyperdoc JSON files</div>
</div>
<ol class="subpoints">
  <li>Hyperdocs grow — they never replace. Each processing pass adds a new layer to the existing hyperdoc, preserving all previous analysis.</li>
  <li>Format version 2 structure: layers[] array (each layer is one processing pass with its own timestamp), cumulative_summary (rolling summary across all layers), format_version field.</li>
  <li>Can create seed hyperdocs for new files before any code is written — capturing the conversation context that led to the file's creation.</li>
</ol>
</div>

<div class="step">
<div class="step-header">
  <span class="step-num">Step 2</span>
  <span class="step-name">Source File Insertion</span>
  <span class="step-file">phase_4_insertion/insert_hyperdocs.py, insert_hyperdocs_v2.py, insert_from_phase4b.py</span>
</div>
<div class="io">
  <div class="input">INPUT: Hyperdoc JSON files + source Python files</div>
  <div class="output">OUTPUT: Enhanced Python files with embedded hyperdoc comments</div>
</div>
<ol class="subpoints">
  <li>Find the insertion point in each source file — after the module docstring, before the first line of actual code.</li>
  <li>Convert the hyperdoc JSON into Python comment blocks using the @ctx annotation format.</li>
  <li>Insert header comments (file-level context), inline function annotations (per-function context placed above each function definition), and footer comments (cross-session summary).</li>
  <li>All existing code is preserved exactly as-is. The insertion is additive only — no lines of code are modified or removed.</li>
</ol>
</div>
</div>

<div class="phase">
<div class="phase-header" style="background:#f85149">
  <span class="phase-num">Phase 5</span>
  <span class="phase-name">Ground Truth</span>
  <span class="phase-cost">$0 (pure Python)</span>
</div>

<div class="step">
<div class="step-header">
  <span class="step-num">Step 1</span>
  <span class="step-name">Claim Extraction</span>
  <span class="step-file">phase_5_ground_truth/claim_extractor.py</span>
</div>
<div class="io">
  <div class="input">INPUT: Hyperdoc comments and grounded markers</div>
  <div class="output">OUTPUT: List of verifiable claims extracted from the hyperdocs</div>
</div>
<ol class="subpoints">
  <li>Parse hyperdoc annotations for claims that can be verified against reality: file existence claims ("this file was created in session X"), function claims ("this function does Y"), test result claims ("all tests pass"), and completion status claims ("this feature is done").</li>
  <li>Each claim is tagged with its source (which hyperdoc, which annotation), its type (existence, function, test, status), and its verifiability level.</li>
</ol>
</div>

<div class="step">
<div class="step-header">
  <span class="step-num">Step 2</span>
  <span class="step-name">Ground Truth Verification</span>
  <span class="step-file">phase_5_ground_truth/ground_truth_verifier.py</span>
</div>
<div class="io">
  <div class="input">INPUT: Extracted claims + actual source code on disk</div>
  <div class="output">OUTPUT: Verification result per claim (VERIFIED, FAILED, UNVERIFIABLE)</div>
</div>
<ol class="subpoints">
  <li>For each claim, check against reality: does the file exist on disk? does the function exist in the file? did the tests actually pass? is the completion status accurate?</li>
  <li>Additional checks: scan for truncation patterns in source code, check for non-Opus model references (Sonnet, Haiku) that violate the Opus-only rule, detect bare except blocks, and identify unsafe API access patterns.</li>
  <li>Each claim gets a result: VERIFIED (reality matches the claim), FAILED (reality contradicts the claim), or UNVERIFIABLE (can't be checked from available data).</li>
</ol>
</div>

<div class="step">
<div class="step-header">
  <span class="step-num">Step 3</span>
  <span class="step-name">Gap Reporting</span>
  <span class="step-file">phase_5_ground_truth/gap_reporter.py</span>
</div>
<div class="io">
  <div class="input">INPUT: Verification results from Step 2</div>
  <div class="output">OUTPUT: Ground truth summary with per-file credibility scores</div>
</div>
<ol class="subpoints">
  <li>Compute credibility score per file: verified claims divided by total verifiable claims. A file with 10 claims and 7 verified gets 70% credibility.</li>
  <li>Identify files with low credibility — these are the files where the hyperdocs are making claims that don't match reality.</li>
  <li>Generate a gap report listing every failed claim with: what the hyperdoc said, what reality shows, and the specific line or file that contradicts the claim.</li>
</ol>
</div>

<div class="step">
<div class="step-header">
  <span class="step-num">Step 4</span>
  <span class="step-name">Schema Normalization</span>
  <span class="step-file">phase_5_ground_truth/schema_normalizer.py</span>
</div>
<div class="io">
  <div class="input">INPUT: All JSON output files across all sessions</div>
  <div class="output">OUTPUT: normalization_log.json — report of schema variants and normalizations applied</div>
</div>
<ol class="subpoints">
  <li>Scan every output JSON file across all session directories for schema consistency.</li>
  <li>Detect schema variants — the same conceptual field stored under different key names across different sessions.</li>
  <li>Normalize field names and structures so all sessions use the same schema, making cross-session queries possible.</li>
  <li>Report how many variants were found and what normalizations were applied.</li>
</ol>
</div>

<div class="step">
<div class="step-header">
  <span class="step-num">Step 5</span>
  <span class="step-name">Completeness Scan</span>
  <span class="step-file">phase_5_ground_truth/completeness_scanner.py</span>
</div>
<div class="io">
  <div class="input">INPUT: All session directories</div>
  <div class="output">OUTPUT: completeness_report.json — which sessions have which phases complete</div>
</div>
<ol class="subpoints">
  <li>Check each session directory for the expected output files from each phase: enriched_session.json (Phase 0), thread_extractions.json + geological_notes.json + semantic_primitives.json + explorer_notes.json (Phase 1), idea_graph.json + synthesis.json (Phase 2), file_dossiers.json (Phase 3).</li>
  <li>Detect stub files — files under 100 bytes that exist but contain no real data.</li>
  <li>Verify field presence in enriched_session.json: check that metadata, filter_tier, filter_signals, and behavior_flags fields are present in the first 5 messages.</li>
  <li>Report incomplete sessions with a list of which phases are missing for each one.</li>
</ol>
</div>
</div>


</body>
</html>